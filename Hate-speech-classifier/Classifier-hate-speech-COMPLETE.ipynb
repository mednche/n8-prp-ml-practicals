{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate speech classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier to recognise hate speech on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to train a 'classifier' (a supervised machine learning algorithm) to recognise hate speech in a tweet. This is a technique of **Natural Language Processing (NLP)** similar to Sentiment Analysis.\n",
    "\n",
    "This requires two phases:\n",
    "- 1. **Train the classifier** (show the algorithm examples of both hateful and non-hateful tweets)\n",
    "- 2. **Test the accuracy of the classifier**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import the libraries we are going to need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nltk version is 3.4.4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/natachachenevoy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/natachachenevoy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import operator \n",
    "import pickle\n",
    "\n",
    "import nltk # nltk 3.4\n",
    "print('The nltk version is {}.'.format(nltk.__version__))\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk import ngrams\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/natachachenevoy/Documents/Github/n8-prp-ml-practicals/Hate-speech-classifier'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.chdir(\"C:/Users/username/.....\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pre-labeled Twitter dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the dataset used to train and test the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tell python where it can find the data files (they're in a `data` sub-directory of the current folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/TrainingTweets.csv', encoding='ISO-8859-1')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `data` is a Pandas dataframe. Let's see what it looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>well yes i mean you started off saying third l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>so my neighbours complained about my shed in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>fucking fascist fucking liberal fucking racist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>fucking annoying when meat dairy and eggs are ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>i hate people i was wrong when i said 97.5 of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  class\n",
       "305  well yes i mean you started off saying third l...      0\n",
       "306  so my neighbours complained about my shed in t...      1\n",
       "307  fucking fascist fucking liberal fucking racist...      1\n",
       "308  fucking annoying when meat dairy and eggs are ...      0\n",
       "309  i hate people i was wrong when i said 97.5 of ...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the last 5 rows of the dataframe\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can see that each tweet has already been manually labeled in the column 'class'.\n",
    "- 1 means hateful\n",
    "- 0 means non-hateful\n",
    "\n",
    "This label is essential for what we are going to do (supervised machine learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Ensure balance in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many hateful vs non-hateful tweets there are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 hateful tweets\n",
      "209 non-hateful tweets\n"
     ]
    }
   ],
   "source": [
    "hate_tweets = data[data[\"class\"] == 1]\n",
    "print(\"{} hateful tweets\".format(len(hate_tweets)))\n",
    "\n",
    "nonhate_tweets = data[data[\"class\"] == 0]\n",
    "print(\"{} non-hateful tweets\".format(len(nonhate_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Muslims go to fucking home bye bye #brexit'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>'Muslim scum terrorists'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>'Go home you immigrant'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>'Polish vermin'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>'Polish bastard'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         tweet  class\n",
       "0  Muslims go to fucking home bye bye #brexit'      1\n",
       "1                     'Muslim scum terrorists'      1\n",
       "2                      'Go home you immigrant'      1\n",
       "3                              'Polish vermin'      1\n",
       "4                             'Polish bastard'      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>'@16po @realDonaldTrump I don\\'t_ terrorism is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>'I am absolutely dreading tomorrow. I hate it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>'RT @washingtonpost: The Rev. William Barber d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>'RT @RedP1llReport: David Icke_ Political Corr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>'My best RTs this week came from: @SkimmySkinn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  class\n",
       "59  '@16po @realDonaldTrump I don\\'t_ terrorism is...      0\n",
       "60  'I am absolutely dreading tomorrow. I hate it ...      0\n",
       "61  'RT @washingtonpost: The Rev. William Barber d...      0\n",
       "62  'RT @RedP1llReport: David Icke_ Political Corr...      0\n",
       "63  'My best RTs this week came from: @SkimmySkinn...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonhate_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be twice as many non-hateful tweets. This might affect the way the algorithm learns. We need to make the dataset balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Select as many hateful as non-hateful tweets for an equal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in balanced dataset: 202\n"
     ]
    }
   ],
   "source": [
    "num = min(len(hate_tweets), len(nonhate_tweets))\n",
    "\n",
    "# shuffle the table of hateful and non hateful tweets\n",
    "hate_tweets = shuffle(hate_tweets)\n",
    "nonhate_tweets = shuffle(nonhate_tweets)\n",
    "\n",
    "data_balanced = hate_tweets[0:num].append(nonhate_tweets[0:num], ignore_index=True)\n",
    "\n",
    "print('Number of tweets in balanced dataset: {}'.format(len(data_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>That man called me a black bastard and to go b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Niggers everywhere these days, make Britain gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>'Fucking paki cunt'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>'Fuck off you back paki bastard'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>These fucking polish aliens, coming here and t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0  That man called me a black bastard and to go b...      1\n",
       "1  Niggers everywhere these days, make Britain gr...      1\n",
       "2                                'Fucking paki cunt'      1\n",
       "3                   'Fuck off you back paki bastard'      1\n",
       "4  These fucking polish aliens, coming here and t...      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we'll perform some preprocessing on the column 'tweet'. It will do things like turn the text all into lowercase, get rid of urls, remove usernames, etc. Have a look at the comments in the code below to see precisely what it's doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(tweet):\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to ''\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    #Remove the RT before the @user \n",
    "    tweet = re.sub('rt','',tweet) \n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #Remove @username\n",
    "    tweet = re.sub('@[^\\s]+','',tweet) \n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Remove non ASCII characters (emojies)\n",
    "    tweet= re.sub(r'[^\\x00-\\x7F]+','', tweet)\n",
    "    #Remove punctuation \n",
    "    tweet = \"\".join(l for l in tweet if l not in string.punctuation)\n",
    "    #Trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    #Remove beginning and end space\n",
    "    tweet = tweet.strip()\n",
    "\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "#  apply cleaning function to each tweet of the pandas dataframe\n",
    "data_balanced['tweet'] = data_balanced['tweet'].apply(cleanTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>that man called me a black bastard and to go b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>niggers everywhere these days make britain gre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>fucking paki cunt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>fuck off you back paki bastard</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>these fucking polish aliens coming here and ta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0  that man called me a black bastard and to go b...      1\n",
       "1  niggers everywhere these days make britain gre...      1\n",
       "2                                  fucking paki cunt      1\n",
       "3                     fuck off you back paki bastard      1\n",
       "4  these fucking polish aliens coming here and ta...      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete empty tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty tweets ('') by NA\n",
    "data_balanced['tweet'].replace('', np.nan, inplace=True)\n",
    "# Delete all NA rows\n",
    "data_balanced.dropna(subset=['tweet'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: How many empty tweets were removed in the process? (Hint: use the shape attribute of the pandas dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tokenise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, the text of each tweet is a string. We would like to separate each word in that string so the model can 'read' them separately. \n",
    "\n",
    "In NLP, this is called 'tokenising': each tweet (intially a string of text) is chopped into a list of tokens (i.e. a list of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokenised = data_balanced.copy()\n",
    "\n",
    "data_tokenised['tweet'] = data_tokenised['tweet'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[that, man, called, me, a, black, bastard, and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[niggers, everywhere, these, days, make, brita...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[fucking, paki, cunt]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[fuck, off, you, back, paki, bastard]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[these, fucking, polish, aliens, coming, here,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0  [that, man, called, me, a, black, bastard, and...      1\n",
       "1  [niggers, everywhere, these, days, make, brita...      1\n",
       "2                              [fucking, paki, cunt]      1\n",
       "3              [fuck, off, you, back, paki, bastard]      1\n",
       "4  [these, fucking, polish, aliens, coming, here,...      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokenised.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can see that there are many words in the tweets that don't bring any meaning such as 'it', 'i', 'of' 'to' etc. These are called stopwords and need to be removed so that the classifier can focus on words that matter when telling the difference between hate and non-hate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Import English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'haven', 'because', 'wouldn', 'by', 'has', 'with', 'any', 'yours', 'both', 'until', 've', 'weren', 'i', 'all', 'then', 'me', \"you've\", 'ourselves', 'had', 'into', \"shan't\", 'some', 'of', 'down', 're', 'been', 'who', 'does', 'same', 'itself', 'on', \"haven't\", 'your', 'more', 'being', 'her', 'why', 'won', \"wouldn't\", 'as', 'out', 'its', 'a', 'the', 'before', 'hadn', 'hasn', 'to', 'wasn', 'did', 'is', 'now', 'over', 'shouldn', 'we', 'was', 'doing', 'which', 'so', 'too', 'can', 'needn', 'isn', 'no', 'mustn', 'his', 'themselves', 'him', 'nor', 'than', \"mustn't\", 'an', 'own', \"didn't\", 'ours', 'myself', 'or', \"won't\", \"wasn't\", 'hers', 'were', \"couldn't\", 'few', 'between', 'd', 'just', \"weren't\", 'have', 'yourself', 'herself', 'yourselves', 'they', 'll', 'didn', 'what', 'are', \"needn't\", 'having', 'each', \"shouldn't\", 'shan', 'these', 'under', 'there', 'such', 'after', \"you'd\", 'off', 'my', 'm', 'y', 'you', 'it', \"don't\", 'doesn', 'during', 'while', 'am', 'mightn', 'aren', 'our', 'whom', \"isn't\", \"hasn't\", 'but', 'them', 'other', 'above', \"hadn't\", 'through', 'he', 'in', 'once', 'further', 'again', \"doesn't\", 'that', 'most', 'if', \"aren't\", 'their', 'ma', 'theirs', 'himself', \"it's\", 'and', 'from', 'how', 'ain', \"mightn't\", \"you'll\", 'do', 'not', 't', 'don', 'those', 'very', 'o', 'about', 'when', 'this', 'only', \"you're\", 'here', \"she's\", 'should', 'she', 'will', \"should've\", 'at', 'up', 'couldn', \"that'll\", 'against', 'be', 's', 'below', 'where', 'for'}\n"
     ]
    }
   ],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these generic English stopwords could actually be useful in our context of hatespeech. For example 'them', 'out', 'off' could all be part of sentences like 'f**k off'. We will take these out of the list of stopwords. Also, we'll add some words to the stopword list based on some common spelling errors we observed in the tweets ('youre', 'dont', 'us')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'aint', 'all', 'am', 'amp', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'cant', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doesnt', 'doing', 'don', \"don't\", 'dont', 'down', 'during', 'each', 'few', 'for', 'further', 'gonna', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'hes', 'him', 'himself', 'his', 'how', 'https', 'i', 'if', 'im', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'over', 'own', 'r', 're', 's', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'shouldnt', 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'thats', 'the', 'their', 'theirs', 'then', 'there', 'theres', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'u', 'under', 'until', 'up', 'ur', 'us', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'wudnt', 'ww', 'y', 'ya', \"you'd\", \"you'll\", \"you're\", \"you're\", \"you've\", 'your', 'youre', 'yours', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "# Remove these from stopwords\n",
    "item_to_delete = ['you', 'out', 'off', 'them', 'themselves', 'yourself', 'from', 'same']\n",
    "stopWords = [e for e in stops if e not in item_to_delete]\n",
    "\n",
    "# Add these to stopwords\n",
    "item_to_add = [\"youre\", \"r\", \"you're\", \"us\", \"doesnt\", \"im\", \"hes\", \"u\", \"ya\", \"ww\", \n",
    "               \"dont\", \"https\", \"aint\", \"theres\", \"shouldnt\", \"thats\", \"amp\", \"wudnt\", \n",
    "               \"gonna\", \"ur\", \"cant\"]\n",
    "for e in item_to_add:\n",
    "    stopWords.append(e)\n",
    "\n",
    "print(sorted(stopWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords from tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to remove these stopwords from the tweets of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the processed data so far\n",
    "data_tokenised_stpwd = data_tokenised.copy()\n",
    "\n",
    "# Apply function that removes stopwords.\n",
    "data_tokenised_stpwd['tweet'] = data_tokenised_stpwd['tweet'].apply(\n",
    "    lambda x: [item for item in x if item not in stopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[man, called, black, bastard, go, back, countr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[niggers, everywhere, days, make, britain, great]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[fucking, paki, cunt]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[fuck, off, you, back, paki, bastard]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[fucking, polish, aliens, coming, taking, jobs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0  [man, called, black, bastard, go, back, countr...      1\n",
       "1  [niggers, everywhere, days, make, britain, great]      1\n",
       "2                              [fucking, paki, cunt]      1\n",
       "3              [fuck, off, you, back, paki, bastard]      1\n",
       "4  [fucking, polish, aliens, coming, taking, jobs...      1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokenised_stpwd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this list of tokens to the one we had prior to removing the stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, stemming is the process of turning words back into their stem, base or root form.\n",
    "\n",
    "Examples:\n",
    "- 'cats' --> 'cat'\n",
    "- 'fishing', 'fished' --> 'fish'\n",
    "\n",
    "This step is important so the classifier understands that the singular and the plural form of a noun carry a similar meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the processed data so far\n",
    "pre_processed_data = data_tokenised_stpwd.copy()\n",
    "\n",
    "ps = PorterStemmer() \n",
    "pre_processed_data['tweet'] = pre_processed_data['tweet'].apply(\n",
    "    lambda x: [ps.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [man, call, black, bastard, go, back, countri,...\n",
       "1       [nigger, everywher, day, make, britain, great]\n",
       "2                                   [fuck, paki, cunt]\n",
       "3                [fuck, off, you, back, paki, bastard]\n",
       "4     [fuck, polish, alien, come, take, job, unaccept]\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_processed_data['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the pre-processing of the 'tweet' column of the dataset. We now have tweets that have been cleaned, stemmed and tokenised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model to learn anything, we need to give it a set of criteria to use in deciding whether a tweet is hateful or not. This kind of criteria is known as **feature**. We can define one or more feature(s) to train our classifier.\n",
    "\n",
    "In Part 2., we'll see how to convert the words into features so that we can feed it to a classifier for training or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Prepare the data to train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What feature shall we give to the model?\n",
    "\n",
    "We could give it a list of key words but text cannot be used by machine learning models. They expect their input to be numeric. So we need to transform words into numeric features in a meaningful way. \n",
    "\n",
    "To do so, we are going to set a list of words/features (called vocabulary) and provide the classifier with boolean values indicating whether each feature of the vocabulary is present or not.\n",
    "\n",
    "It will look something like this:\n",
    "- 'bastard' : True (present)\n",
    "- 'road' : False (absent)\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by creating a vocabulary of features: a set of words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 664 unique words\n"
     ]
    }
   ],
   "source": [
    "list_features = [word for tweet in pre_processed_data['tweet'] for word in tweet]\n",
    "\n",
    "vocab = set(list_features)\n",
    "print('Vocabulary size: {} unique words'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'proud', 'delici', 'perv', 'prospect', 'na', 'europ', 'reggi', 'sat', 'cool', 'johansson', 'resid', 'ad', 'squid', 'sun', 'deal', 'fethullahyd', 'long', 'crisp', 'bad', 'local', 'violet', 'xfactor', 'polish', 'home', 'season', 'terror', 'myriah', 'version', 'later', 'let', 'xxxx', 'japan', 'rockit', 'central', 'differ', 'hors', 'univers', 'know', 'jacksonvil', 'paleo', 'nearli', 'margat', 'wear', 'physic', 'bunch', 'chill', 'babi', 'nail', 'cake', 'racist', 'extra', 'face', '3', 'high', 'much', 'probabl', 'compani', 'internet', 'accomplish', 'aug', 'sinc', 'vintag', 'black', 'yr', 'soon', 'callin', 'tame', 'box', 'church', 'johnni', 'never', 'vegan', 'winckley', 'hard', 'tire', 'link', 'watch', 'ab', 'sub', 'facebook', 'plan', 'ill', 'along', 'diet', 'five', 'lancast', 'everyth', 'well', 'smh', 'sli', 'muslim', 'correctnessbut', 'learn', 'krtk', 'hate', 'alien', 'chanyeol', 'shop', 'terrorist', 'ago', 'you', 'liber', 'follow', 'natur', 'garden', 'arent', 'ok', 'star', 'smile', '102', 'men', 'worri', 'but1', 'nuala', 'thank', 'j', 'cheat', 'broke', 'wog', 'today', 'sit', 'run', 'rid', 'spicskkk', 'point', 'palm', 'day', 'woh', 'fascist', 'want', 'green', 'lectur', 'lmfao', 'fear', 'great', 'allow', 'id', 'hasnt', 'get', 'unaccept', 'firefli', 'hoursdishoomincinema', 'meme', 'alarm', 'anticip', 'disgrac', 'steph', 'scarlett', 'fag', 'forb', 'a4b', 'fine', 'fat', 'second', 'wont', 'strung', 'inconsider', 'pygmi', 'behind', 'fulli', '39', 'understand', 'oh', 'achiev', 'coast', 'anim', 'fucker', 'realiz', 'annoy', 'rude', '56', 'oppoun', 'poignantli', 'kill', 'maniq', 'offer', 'leagu', 'respect', 'graffiti', 'shut', 'suppo', 'think', 'sight', 'spain', 'yall', 'cinema', 'xxx', 'eu', 'food', 'difficult', 'mani', 'joshuawhyt', 'count', 'sta', 'becom', 'baskn', 'american', 'm6m61', 'compil', 'thick', 'front', 'pleas', 'round', 'chav', 'remind', 'kick', 'treehugg', 'funni', 'same', 'rip', 'meat', 'scream', 'right', '10', 'complet', 'deadlock', 'dyke', 'chest', 'tri', 'out', 'breastfeed', 'crazi', 'weekend', 'show', 'forget', 'steal', 'updat', 'tank', 'still', 'london', 'like', 'fail', 'stop', 'find', 'tonight', 'higher', 'condition', 'discount', 'nowplay', 'one', 'take', 'nigger', 'mostli', 'make', 'chink', 'daddi', 'egg', 'amaz', 'countri', 'au', 'shed', 'ribchest', 'januari', 'hous', 'imagin', 'end', 'bryan', 'femal', 'mother', 'danger', 'juandaearli', 'trump', 'yourself', 'write', 'anymor', 'guy', 'looool', 'baekhyun', 'rudolph', 'orangi', 'gazet', 'treat', 'allah', 'achrostmad', 'fyld', 'bit', 'emxzf', 'fit', 'bodi', '15', 'cours', 'complain', 'appli', 'rumbl', 'mamp', 'sheboon', 'sand', 'kinda', 'rhose', 'news', 'irish', 'gon', 'rasslekast', 'die', 'tomato', 'foreign', 'ankl', 'de', 'stay', 'drunk', 'life', 'deserv', 'littl', 'rain', 'metal', 'vermin', 'trythesewrestlingfanpodcast', 'decis', 'absolut', 'okay', 'from', 'scare', 'night', 'cleveleysno', 'crep', 'need', 'hotel', 'promis', 'didnt', 'decencyampcommon', 'freedom', 'petrolhead', 'tomorrow', 'least', 'realiti', 'burn', 'morn', 'england', 'here', 'caller', 'last', 'ryan', 'avfc', 'chang', 'tourou', 'colour', 'robot', 'text', 'technolog', 'kenchan', 'storag', 'self', 'lilli', 'awkward', 'new', 'cut', 'citicar', 'dairi', 'problem', 'heaoverh', 'ugli', 'benjamin', 'moron', 'lover', 'drag', 'facil', 'cover', 'jar', 'spider', 'kondobyjaymoni', 'boost', 'rest', 'simon', 'ass', 'skinni', 'record', 'bush', 'scarlettjohansson', 'british', 'moment', 'thankyou', 'onbsc2016', 'place', 'bird', '3sec2', 'dog', 'wanker', 'chavcentr', 'advic', 'prematur', 'drop', 'test', 'bitch', 'slagfound', 'faggot', 'advantag', 'someon', 'bk', 'girl', 'custom', 'shredder', 'arm', 'everywher', 'hilari', 'bastard', 'train', 'ep', 'databas', 'preston', 'smoke', 'pork', 'outreach', 'detail', 'anyon', 'give', 'cunt', 'joke', 'usconstitut', 'town', 'love', '117', 'meet', 'merri', 'leav', 'prepar', 'ego', 'time', 'pretti', 'iq', 'dude', 'poke', 'though', 'head', 'gold', 'sens', 'tell', 'buy', 'fagett', 'sponsor', 'game', 'click', '321', 'irelandfew', 'fight', 'till', 'britain', '36', 'outsid', 'climb', 'yet', 'year', 'job', 'keep', 'happen', 'scum', 'brazil', 'even', 'fab', 'scratch', 'x', 'believ', 'p', 'shi', '2', 'two', 'exampl', 'cba', 'brexit', 'road', 'paul', '975', 'see', 'mosqu', 'them', 'daughter', 'retweet', 'sooo', 'glad', 'seen', 'beat', 'ye', 'christma', 'neighbour', 'leader', 'oreo', 'wayn', 'finger', 'oran', 'lol', 'moral', 'ralli', 'hand', 'saw', 'citizenship', 'sale', 'look', 'good', 'reason', 'polit', 'download', 'mitch', 'chutney', 'starr', 'twat', 'uncoupl', 'doo', 'chelsea', 'jefre', 'minut', 'email', 'break', 'god', 'diy', '50', 'hold', 'steve', 'spread', 'bye', 'call', 'hour', 'better', 'roman', 'golf', 'who', 'shout', 'attent', 'leg', 'sho', 'careerarc', 'gel', '1488', 'scoobi', 'prick', 'bother', 'bearwasnt', 'tree', 'braveri', 'white', 'scottish', 'dread', 'enjoy', 'fact', 'mcconnel', 'veil', 'expand', 'enough', 'monkey', 'product', 'sexist', 'breaktim', 'sleep', 'religion', 'snap', 'smelli', 'akp', 'citi', 'summer', 'poland', 'cow', '07', 'whitepow', 'mate', 'akbar', 'realli', 'strip', 'might', 'fuck', 'inspir', 'immigr', 'lost', 'wrong', 'pay', 'hope', 'finish', 'man', 'villa', 'common', 'economi', 'car', 'word', 'oppo', 'preorder', 'world', 'rape', 'buddi', 'seek', 'oti', 'fl', 'way', 'sock', 'yellow', 'bear', 'justwond', 'happi', 'awwir', 'readi', 'glitteri', 'shithol', 'clevli', 'come', 'elderrumb', 'hell', 'father', 'pa', 'arriv', 'off', 'vs', 'turn', 'went', '0230', 'ex', 'clearli', 'throne', 'accept', 'wrea', 'rock', 'topstori', '1', 'traitor', 'bloer', 'hire', 'put', 'despit', 'disgust', 'wld', 'onlin', 'dig', 'polic', 'around', 'post', 'next', 'whore', 'spooki', 'famili', 'joshua', 'tie', 'decid', 'work', 'crash', 'dramat', 'bihday', 'select', 'thing', 'featur', 'jezani', 'paki', 'hahah', 'say', 'cakepop', 'back', 'tule', 'peopl', 'use', 'shit', 'health', 'tune', 'gotten', 'cafe', 'go', 'figur', '128514', 'latino', 'school', 'said', 'bruh', 'cbb', 'visa', 'stupid', 'depo', '20', 'recolorshad', 'truli', 'beauti', 'research'}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vocabulary contains a list of all unique words in our pre-processed tweets. You'll notice some of the words don't look very english. It's because they are the stem of the initial word (recall the stemming process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of hate in the English language is more complex than just the presence of a word. Sometimes it's the combination of 2 or more words that becomes hateful. For example 'shut up', 'f**k off' or 'send them home'. \n",
    "\n",
    "In NLP, these combinations of 2 or more words are called ngrams:\n",
    "- bigram: ('back', 'off')\n",
    "- trigram: ('send', 'them', 'home')\n",
    "\n",
    "We need to add bigrams and trigrams to our vocabulary of features alongside single words/features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2443\n"
     ]
    }
   ],
   "source": [
    "def get_list_all_tokens(tweets):\n",
    "    all_words = []\n",
    "    for word_list in tweets:\n",
    "        # unigrams\n",
    "        all_words.extend(word_list)\n",
    "        \n",
    "        # bigrams\n",
    "        bigrams = list(ngrams(word_list, 2))\n",
    "        \n",
    "        #trigrams \n",
    "        trigrams = list(ngrams(word_list, 3))\n",
    "        \n",
    "        all_words.extend(bigrams)\n",
    "        all_words.extend(trigrams)\n",
    "    \n",
    "    return all_words\n",
    "\n",
    "list_features = get_list_all_tokens(pre_processed_data['tweet'])\n",
    "\n",
    "vocab = set(list_features)\n",
    "print('Vocabulary size: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('around', 'scream', 'allah'), ('hope', 'you'), 'delici', ('black', 'bastard'), 'perv', ('spider', 'cakepop', 'out'), ('might', 'chang'), ('you', 'polish'), ('muslim', 'go'), ('muslim', 'go', 'fuck'), ('irish', 'cow'), ('good', 'peopl'), ('2', 'differ', 'hors'), ('smile', 'let'), 'fethullahyd', ('car', 'crash', 'compil'), ('liber', 'fuck', 'racist'), ('leav', 'europ'), 'violet', ('gazet', 'krtk', 'baskn'), ('cunt', 'much'), ('plan', 'never', 'happen'), ('smile', 'chang', 'world'), ('one', 'ok', 'rock'), 'version', ('want', 'fuck', 'daughter'), ('muslim', 'scum', 'terrorist'), ('forget', 'get', 'jar'), ('god', 'bother', 'cunt'), ('absolut', 'dread'), 'univers', ('seen', 'strip'), ('back', 'countri'), 'physic', ('hate', 'even', 'yet'), 'nail', ('polish', 'vermin'), 'face', ('immigr', 'appli'), ('36', 'hoursdishoomincinema'), ('imagin', 'train'), ('fag', 'fagett', 'femal'), ('moron', 'trump'), ('poignantli', 'point', 'out'), ('tourou', 'anim', 'id'), ('citi', 'hire', 'jacksonvil'), ('2', 'year', 'winckley'), ('gazet', 'krtk'), ('you', 'deserv'), 'never', ('two', 'internet', 'decid'), ('ep', '39', 'day'), 'hard', ('rumbl', 'joshua', 'forget'), ('shut', 'nigger'), ('local', 'paki'), 'along', ('summer', 'plan', 'never'), ('onlin', 'emxzf'), 'five', ('get', 'rid'), 'well', ('peopl', 'you'), ('bk', 'tri'), ('achiev', 'give'), ('uncoupl', 'record'), ('american', 'work'), ('rid', 'them', 'along'), ('ab', 'gotten', 'realli'), ('you', 'you'), 'star', 'you', 'follow', 'garden', ('you', 'even', 'countri'), 'arent', ('chest', 'seen'), 'men', ('na', 'recolorshad'), ('put', 'american'), ('hors', 'spread'), ('yourself', 'you', 'cunt'), 'nuala', 'thank', ('fuck', 'annoy', 'meat'), 'cheat', ('countri', 'hous', 'hotel'), ('well', 'last'), ('out', 'you'), ('game', 'throne'), ('sho', 'outreach'), 'point', ('jefre', 'starr'), ('like', 'realiz'), ('compil', 'pa', '321'), ('yr', 'bk'), 'woh', ('you', 'white'), 'great', 'allow', ('hous', 'hotel', 'wrea'), ('rhose', 'word'), ('bastard', 'immigr', 'keep'), ('bastard', 'muslim', 'terrorist'), ('get', 'out'), ('get', 'back'), 'alarm', 'anticip', 'disgrac', ('recolorshad', 'head', 'crep'), ('you', 'fuck'), ('look', 'like'), ('news', 'tell'), ('brexit', 'time', 'fuck'), ('like', '2'), ('fascist', 'fuck'), ('traitor', 'fuck', 'petrolhead'), 'inconsider', ('hate', 'yellow', 'fucker'), 'behind', 'fulli', 'oh', ('nearli', 'much'), ('decencyampcommon', 'sens'), ('smile', 'let', 'smile'), ('out', 'hand', 'nuala'), ('back', 'featur'), 'rude', 'oppoun', ('mamp', 'cleveleysno'), ('say', 'face', 'behind'), ('off', 'you', 'cunt'), ('rape', 'one'), ('remind', 'prospect', 'arent'), 'respect', ('brazil', 'beauti', 'place'), ('moment', 'you'), ('american', 'give', 'citizenship'), ('au', 'natur'), ('show', 'nowplay'), 'think', ('paul', 'ryan', 'mitch'), 'cinema', ('off', 'immigr', 'brexit'), ('girl', 'johnni'), ('post', 'facebook'), ('racist', 'fuck'), 'count', ('off', 'home', 'you'), 'baskn', ('depo', 'bastard'), ('follow', 'topstori', 'spain'), 'thick', ('someon', 'tell'), ('email', 'get', 'you'), ('dude', 'smoke'), ('get', 'enough', 'pork'), ('fag', 'like', 'fagett'), 'remind', ('rest', 'good'), ('leav', 'food'), ('see', 'ad'), ('tomorrow', 'hate', 'even'), 'funni', 'same', 'rip', ('fucker', 'leav'), ('hahah', 'wont'), ('countri', 'you', 'white'), ('promis', 'p'), ('arriv', 'town', 'five'), ('you', 'foreign'), ('spooki', 'spider', 'cakepop'), ('sight', 'accomplish', 'from'), 'chest', 'out', ('right', 'time'), ('fuck', 'terrorist', 'everywher'), ('leg', 'break', 'smh'), ('immigr', 'keep', 'britain'), ('lilli', 'tri'), ('shit', 'out'), 'like', ('rockit', 'round', 'good'), 'fail', 'stop', ('bye', 'brexit'), ('shout', 'go'), 'tonight', ('give', 'extra'), ('you', 'fuck', 'foreign'), ('outsid', 'leav'), ('citizenship', 'time'), 'one', 'nigger', 'daddi', 'amaz', ('seen', 'strip', 'second'), ('oh', 'god', '128514'), ('attent', 'seek'), ('tri', 'give'), ('tomorrow', 'love'), ('detail', 'job'), ('na', 'cut'), ('went', 'off'), ('realli', 'enjoy', 'box'), ('yellow', 'fucker'), ('end', 'you'), ('56', 'sponsor'), ('outreach', 'email', 'get'), ('fuck', 'polish', 'alien'), 'januari', ('compani', 'still'), ('scream', 'allah'), ('like', 'jefre'), ('enough', 'pork'), ('go', 'hous', 'xxxx'), 'bryan', 'femal', ('smile', 'chang'), ('glitteri', 'star'), 'trump', ('smoke', 'reggi'), ('polish', 'off'), ('bastard', 'get'), ('coast', 'green', 'tomato'), 'guy', ('everyth', 'you'), ('new', 'ep'), 'orangi', ('nigger', 'everywher'), 'fyld', 'emxzf', ('you', 'post', 'fine'), ('like', 'you'), ('black', 'slagfound'), 'complain', ('annoy', 'meat', 'dairi'), ('deal', 'understand', 'know'), ('town', 'five', 'hour'), ('nigger', 'whore'), ('good', 'peopl', 'you'), ('come', 'make'), ('religion', 'fuck'), ('wanker', 'you'), ('back', 'faggot'), 'ankl', ('you', 'ass', 'prick'), ('want', 'go', 'hous'), 'life', ('polit', 'deadlock', 'forb'), ('awwir', 'rasslekast'), ('bitch', 'road', 'need'), ('two', 'internet'), ('bitch', 'fuck', 'off'), ('prepar', 'one'), ('today', 'said'), ('hand', 'nuala'), ('tie', 'end', 'nigger'), ('hope', 'get', 'rape'), ('go', 'home', 'oreo'), 'freedom', ('weekend', 'take'), ('sale', 'kick', 'off'), ('world', 'heaoverh'), ('mani', 'peopl', 'you'), ('nigger', 'lmfao'), ('decid', 'fuck'), ('good', 'buddi', 'resid'), ('na', 'cut', 'bitch'), ('gel', 'polish', 'natur'), 'chang', ('work', 'out'), ('fail', 'oh'), 'colour', ('town', 'five'), 'kenchan', ('time', 'test', 'advic'), ('fuck', 'white'), 'self', ('yall', 'proud'), 'lilli', ('god', 'make'), ('07', 'despit', 'polit'), ('bastard', 'wog', 'road'), 'cut', ('chelsea', 'ye'), ('immigr', 'fuck', 'terrorist'), ('twat', 'm6m61', 'way'), ('advic', 'becom'), 'citicar', ('fat', 'littl'), ('juandaearli', 'caller', 'learn'), ('bastard', 'immigr'), ('off', 'road'), 'lover', ('scarlett', 'johansson'), ('10', 'make', 'okay'), ('off', 'you', 'back'), ('fail', 'oh', 'god'), 'spider', 'kondobyjaymoni', ('tree', 'bird', 'complet'), ('out', 'countri'), ('hous', 'xxxx'), 'record', 'british', 'place', ('them', 'palm', 'hand'), ('roman', '117'), ('hotel', 'wrea', 'green'), 'advic', ('0230', 'didnt', 'know'), ('fine', 'exampl'), ('road', 'better', 'stop'), ('neighbour', 'church'), 'test', ('technolog', 'decis'), 'slagfound', ('new', 'home'), 'bk', ('polish', 'bitch', 'fuck'), ('turn', 'back'), ('face', 'behind', 'back'), ('achiev', 'give', 'chill'), 'arm', 'everywher', ('akp', 'de', 'fethullahyd'), ('cheat', 'fuck'), 'ep', ('paki', 'perv'), ('go', 'go', 'preorder'), 'outreach', ('na', 'recolorshad', 'head'), 'anyon', 'detail', 'joke', ('polit', 'correctnessbut', 'love'), 'town', ('you', 'go'), ('golf', 'cours', 'fl'), 'dude', ('paki', 'cunt'), ('resid', 'shredder'), ('last', 'time', 'preston'), ('new', 'ep', '39'), ('you', 'understand', 'you'), ('you', 'post'), ('nuala', 'nowplay', 'sun'), ('advic', 'becom', 'physic'), ('girl', 'johnni', 'oti'), 'click', ('time', 'preston', 'anymor'), ('littl', 'wanker'), ('text', 'back'), ('faggot', 'like', 'you'), 'britain', ('world', 'chang', 'smile'), ('you', 'daddi'), ('despit', 'polit', 'deadlock'), 'scum', ('shop', 'disgust'), ('bad', '1'), 'x', ('night', 'fag', 'fagett'), 'shi', ('off', 'new', 'home'), '2', ('firefli', 'squid'), ('mani', 'delici'), ('everywher', 'day', 'make'), ('you', 'back'), ('deadlock', 'forb'), ('krtk', 'baskn', 'oran'), 'paul', ('cunt', 'fuck'), ('oppo', 'good'), ('tell', 'them', 'thing'), ('suppo', 'off'), ('go', 'fuck', 'home'), ('extra', 'ankl'), ('let', 'smile', 'chang'), ('day', 'go'), ('visa', 'immigr', 'work'), ('along', 'europ'), ('perv', 'clearli', 'want'), ('cunt', 'much', 'much'), ('callin', 'daddi', 'daddi'), ('fuck', 'off', 'myriah'), 'leader', ('product', 'from', 'leav'), ('clearli', 'want', 'fuck'), 'finger', ('meet', 'j'), 'moral', ('102', '10'), ('databas', 'respect'), ('15', 'reason'), ('mother', 'juandaearli', 'caller'), ('work', 'golf', 'cours'), ('point', 'out', 'disgrac'), 'hand', ('brazil', 'beauti'), 'saw', 'reason', ('you', 'usconstitut'), ('funni', '3'), ('get', 'visa'), ('bird', 'complet'), ('terrorist', 'everywher', 'get'), ('need', 'out'), ('think', 'ok'), ('avfc', 'preston'), ('least', 'look'), ('back', 'bird'), ('point', 'moron'), ('paul', 'ryan'), ('dude', 'smoke', 'reggi'), ('look', 'like', 'jefre'), ('fascist', 'fuck', 'liber'), 'chelsea', 'jefre', ('juandaearli', 'caller'), ('seek', 'bitch'), ('time', 'test'), 'break', ('great', 'fight'), ('out', 'chanyeol', 'achiev'), ('mosqu', 'let', 'terrorist'), ('them', 'mamp'), ('realli', 'skinni'), ('cours', 'fl'), ('home', 'today', 'ribchest'), ('akp', 'de'), ('self', 'storag'), ('kill', 'you'), ('poke', 'finger'), 'roman', ('home', 'you', 'immigr'), ('countri', 'girl'), ('box', 'tonight'), 'golf', ('know', 'fuck', 'go'), ('daddi', 'wont'), 'who', ('polish', 'ego'), ('kill', 'you', 'you'), ('becom', 'physic'), ('go', 'lectur'), ('spicskkk', 'ralli'), ('job', 'citicar'), ('let', 'go'), ('black', 'nigger'), ('poignantli', 'point'), 'braveri', 'dread', 'enjoy', ('allah', 'akbar', 'terrorist'), ('out', 'off'), ('cunt', 'wear', 'veil'), ('time', 'leav'), 'mcconnel', ('tomato', 'chutney'), 'monkey', ('put', 'american', 'work'), ('complet', 'fail', 'oh'), 'product', ('polit', 'deadlock'), ('id', 'love'), 'akp', ('enough', 'fuck', 'paki'), 'citi', ('love', 'follow'), 'cow', ('even', 'yet'), ('home', 'gon'), ('off', 'back'), ('ad', 'crisp', 'cover'), ('exampl', 'braveri', 'inspir'), ('fuck', 'off', 'pleas'), ('much', 'much', 'higher'), ('countri', 'man'), ('mitch', 'mcconnel'), ('3sec2', 'funni'), 'economi', ('post', 'facebook', '15'), ('hell', 'put', 'american'), ('leav', 'fuck'), ('go', 'christma'), ('wrong', 'said', '975'), ('you', 'nigger'), 'rape', ('baekhyun', 'work'), ('everywher', 'day'), ('you', 'muslim', 'prick'), ('round', 'shut'), ('babi', 'boost'), 'way', 'sock', ('buy', 'you'), 'justwond', ('shut', 'nigger', 'whore'), ('high', 'immigr', 'appli'), ('tame', 'bearwasnt', 'hard'), ('fuck', 'white', 'bitch'), ('faggot', 'sit'), ('man', 'pleas'), ('scare', 'father', 'christma'), ('hold', 'shi', 'chest'), ('take', 'immigr', 'poignantli'), ('id', 'polish', 'off'), ('out', 'fine'), ('britain', 'great'), ('oppo', 'good', 'buddi'), ('bitch', 'next'), ('fab', 'id'), ('squid', 'japan'), ('end', 'januari', 'go'), 'pa', 'arriv', ('outsid', 'leav', 'food'), ('good', 'news'), 'turn', ('said', '975', 'peopl'), ('enough', 'pork', 'scratch'), ('understand', 'know', 'someon'), ('tri', 'climb', 'tree'), ('complain', 'shed', 'front'), 'bloer', ('ryan', 'mitch', 'mcconnel'), 'put', ('watch', 'season', 'two'), ('paki', 'peopl', 'respect'), ('egg', 'tourou', 'anim'), ('didnt', 'know', 'fuck'), ('fuck', 'paki', 'man'), 'post', ('fulli', 'them'), ('you', 'problem'), ('hate', 'polit', 'correctnessbut'), ('fuck', 'off', 'immigr'), ('smelli', 'paki'), ('paki', 'man'), ('drag', 'freedom'), ('databas', 'respect', 'uncoupl'), ('hope', 'you', 'strung'), 'hahah', 'paki', ('hate', 'immigr'), ('uncoupl', 'record', 'onlin'), ('you', 'paki', 'bastard'), ('spider', 'cakepop'), ('hate', 'leav', 'guy'), ('trythesewrestlingfanpodcast', 'awwir', 'rasslekast'), ('come', 'make', 'rain'), ('20', 'brazil'), ('man', 'time', 'leav'), ('fuck', 'immigr'), ('gel', 'polish'), ('back', 'paki', 'bastard'), ('keep', 'see', 'ad'), ('cake', 'sale'), ('leagu', 'meme'), ('anim', 'id'), ('go', 'hous'), 'stupid', ('you', 'polish', 'alien'), ('good', 'night'), ('still', 'hasnt', 'kick'), ('littl', 'wanker', 'you'), ('terrorist', 'out'), ('garden', 'who'), 'beauti', ('975', 'peopl', 'cunt'), ('leagu', 'meme', 'hilari'), ('home', 'bye'), ('chav', 'central'), 'prospect', ('make', 'britain'), ('shredder', 'from', 'rockit'), 'reggi', ('nigger', 'bruh'), ('you', 'ass'), 'sat', ('finger', 'snap'), 'cool', ('fuck', 'bitch'), 'johansson', ('fight', 'joshuawhyt', 'let'), 'resid', 'ad', 'squid', 'sun', 'deal', ('paki', 'man', 'time'), 'bad', ('like', 'poke', 'finger'), ('good', 'egg', 'get'), ('wont', 'you', 'daddi'), ('countri', 'man', 'pleas'), ('follow', 'paleo'), ('call', 'black', 'bastard'), ('know', 'someon'), 'terror', ('nigger', 'hate', 'faggot'), ('strip', 'second', 'ago'), ('food', 'rudolph'), 'later', 'let', ('hate', 'you', 'nigger'), 'japan', ('thankyou', 'you', 'sock'), ('train', 'prepar'), ('eu', 'sat'), ('thick', 'scottish'), 'know', 'jacksonvil', ('behind', 'back'), ('second', 'ago'), ('chest', 'seen', 'strip'), 'wear', 'chill', ('hard', 'you', 'polish'), 'cake', ('racist', 'fuck', 'traitor'), ('hell', 'put'), 'racist', ('cafe', 'clevli'), ('bear', 'arm', 'kill'), ('fuck', 'stupid'), '3', ('work', 'like'), 'compani', ('work', 'yet', 'get'), ('arriv', 'town'), ('suppo', 'off', 'road'), ('go', 'fuck'), 'accomplish', ('pygmi', 'paul'), 'aug', 'vintag', 'black', ('bear', 'arm'), 'callin', ('back', '3sec2', 'funni'), ('get', 'out', 'countri'), ('hate', 'peopl', 'wrong'), ('sub', 'ye', 'sub'), ('hate', 'bitch'), ('day', 'go', 'go'), 'watch', ('de', 'fethullahyd'), ('go', 'back', 'countri'), ('advantag', '10'), ('egg', 'get'), ('out', 'countri', 'you'), 'ill', ('chang', 'world'), ('place', 'anticip'), 'sli', ('terrorist', 'shit'), ('fuck', 'faggot'), 'correctnessbut', 'learn', ('let', 'terrorist'), ('fuck', 'off'), ('daddi', 'daddi'), ('back', 'shithol', 'go'), ('bastard', 'wog'), ('scottish', 'bastard'), ('want', 'work', 'citi'), 'smile', '102', ('immigr', 'fact', 'bodi'), ('alarm', 'went', 'off'), ('anymor', 'inconsider'), ('golf', 'cours'), ('akbar', 'terrorist', 'shit'), ('give', 'chill'), ('muslim', 'terrorist'), ('you', 'nigger', 'bruh'), ('diet', 'danger', 'vegan'), ('scum', 'game'), 'j', ('rip', 'summer', 'plan'), ('scum', 'terrorist'), 'today', ('decis', 'leader'), ('off', 'amaz'), ('off', 'coast', 'irelandfew'), ('10', 'make'), ('fuck', 'jezani', 'cunt'), 'green', ('immigr', 'fact'), ('road', 'need', 'out'), ('countri', 'you', 'paki'), 'id', 'hasnt', ('accomplish', 'from'), ('right', 'sight'), ('rip', 'summer'), ('nigger', 'sheboon', 'hope'), ('depo', 'bastard', 'immigr'), ('hors', 'spread', 'leg'), ('you', 'get'), ('england', 'bunch'), 'forb', 'a4b', ('fact', 'work', 'out'), ('learn', 'hate'), ('right', 'self'), 'second', 'strung', 'pygmi', 'achiev', 'anim', ('coast', 'green'), ('around', 'truli', 'want'), ('star', 'you'), ('man', 'call'), ('home', 'you', 'foreign'), 'annoy', ('tell', 'them'), '56', ('fuck', 'countri', 'you'), ('rumbl', 'joshua'), ('season', 'two'), ('terror', 'bad'), ('fuck', 'go'), ('fuck', 'treehugg'), ('you', 'never'), ('tie', 'end'), 'shut', ('year', 'winckley'), 'spain', ('you', 'strung'), ('p', 'dude'), ('mani', 'spooki'), ('like', 'think'), ('vintag', '2', 'year'), 'joshuawhyt', ('terrorist', 'rest'), ('leg', 'break'), 'sta', ('like', 'poke'), ('rid', 'them'), 'm6m61', 'compil', ('buy', 'you', 'food'), ('call', 'paki'), ('hold', 'let', 'go'), ('daddi', 'come'), ('you', 'love', 'someon'), ('off', 'myriah'), ('advantag', '10', 'off'), ('colour', 'fucker'), ('well', 'soon'), ('make', 'britain', 'great'), ('accomplish', 'from', 'sta'), ('oppoun', 'pay'), ('food', 'rudolph', 'thankyou'), ('never', 'turn'), 'scream', ('sit', 'cinema', 'next'), ('you', 'deserv', 'everyth'), ('absolut', 'dread', 'tomorrow'), ('fuck', 'irish', 'cow'), ('might', 'chang', 'tune'), ('job', 'steal', 'immigr'), ('make', 'rain'), 'weekend', 'steal', ('peopl', 'wrong', 'said'), ('back', '3sec2'), ('oran', 'akp', 'de'), ('white', 'bitch', 'road'), ('inconsider', 'twat'), 'find', ('home', 'today'), ('paki', 'shop'), ('leav', 'fuck', 'countri'), ('updat', 'databas', 'respect'), ('come', 'out', 'you'), 'discount', ('disgust', 'even'), ('go', 'fuck', 'yourself'), 'take', ('sexist', 'perv', 'clearli'), ('problem', 'say'), ('paki', 'shop', 'disgust'), ('sinc', 'simon', 'decid'), ('right', 'sight', 'accomplish'), ('even', 'countri', 'you'), ('fuck', 'paki', 'cunt'), ('gotten', 'realli'), ('akbar', 'terrorist'), ('tonight', 'great'), ('femal', 'version', 'fag'), ('realli', 'enjoy'), ('mani', 'spooki', 'spider'), 'imagin', ('saw', 'graffiti'), ('off', 'avfc', 'preston'), ('them', 'along', 'europ'), ('bryan', 'new'), ('pork', 'scratch', 'minut'), 'juandaearli', ('anyon', 'british'), ('out', 'fine', 'tomorrow'), ('pretti', 'funni'), ('resid', 'shredder', 'from'), 'looool', 'rudolph', ('them', 'bastard', 'muslim'), ('neighbour', 'church', 'england'), 'allah', 'achrostmad', ('summer', 'plan'), 'bit', 'fit', ('crash', 'compil', 'pa'), 'bodi', ('sock', 'give', 'extra'), ('nigger', 'whitepow'), ('product', 'from'), ('come', 'home'), ('delici', 'treat'), ('oti', 'show'), ('muslim', 'prick'), ('take', 'job', 'unaccept'), 'irish', ('go', 'drag'), ('leg', '2'), ('scoobi', 'doo', 'dog'), ('still', 'hasnt'), 'foreign', ('here', 'oppo'), ('countri', 'rude'), 'drunk', ('select', 'right', 'self'), ('them', 'bastard'), ('time', 'right'), ('offer', 'oppoun'), 'deserv', ('wog', 'leav', 'countri'), ('around', 'truli'), 'decis', 'absolut', ('drop', 'dramat', 'sinc'), 'okay', ('want', 'fuck'), ('road', 'need'), ('wayn', 'x'), ('post', 'fine'), ('steve', 'you', 'steph'), ('probabl', 'gon'), ('forget', 'get'), ('time', 'leav', 'europ'), ('tourou', 'anim'), 'least', ('american', 'work', 'yet'), ('shit', 'diy'), 'morn', ('fuck', 'paki'), ('arm', 'kill', 'mani'), 'england', 'caller', 'ryan', 'avfc', ('nail', 'violet'), ('immigr', 'appli', 'mostli'), ('select', 'right'), 'text', 'technolog', 'storag', ('right', 'self', 'storag'), ('scream', 'allah', 'akbar'), 'dairi', ('heaoverh', 'kenchan'), 'problem', 'heaoverh', 'ugli', ('enjoy', 'box', 'tonight'), ('steal', 'immigr'), ('jacksonvil', 'fl', 'click'), ('lol', 'fear'), 'rest', ('femal', 'version'), ('you', 'fuck', 'paki'), 'boost', 'skinni', ('news', 'tell', 'god'), ('understand', 'know'), ('pay', 'you', '50'), 'bush', 'scarlettjohansson', ('femal', 'hate', 'you'), ('fuck', 'dyke'), ('famili', 'breastfeed'), ('nail', 'violet', '102'), ('lancast', 'download'), ('research', 'find'), ('ye', 'sub', 'off'), 'bitch', ('get', 'them', 'mamp'), 'advantag', 'girl', ('go', 'x'), ('bloer', 'cafe'), 'hilari', 'bastard', 'train', 'databas', 'preston', ('dread', 'tomorrow'), ('allah', 'akbar'), ('good', 'news', 'tell'), ('kinda', 'glad', 'difficult'), ('tell', 'god'), ('you', 'problem', 'say'), ('you', 'sock', 'give'), ('fuck', 'daughter', 'disgust'), 'usconstitut', ('spread', 'leg', 'break'), 'meet', ('countri', 'hous'), 'leav', ('cover', 'gold'), ('anymor', 'inconsider', 'twat'), 'time', ('hour', 'go', 'lectur'), ('home', 'off'), ('back', 'featur', 'lost'), 'poke', ('shout', 'go', 'round'), ('fuck', 'anyon', 'british'), ('funni', '3', 'ill'), ('snap', 'tule', 'tank'), ('out', 'you', 'black'), ('thank', 'you'), ('said', 'fuck'), 'tell', ('christma', 'even', 'though'), 'fagett', ('m6m61', 'way'), ('p', 'dude', 'smoke'), 'game', ('ill', 'buy', 'you'), 'irelandfew', 'fight', ('fuck', 'stupid', 'ass'), ('peopl', 'you', 'accept'), 'till', ('steve', 'you'), ('johnni', 'oti', 'show'), ('colour', 'fucker', 'leav'), ('much', 'higher'), ('dread', 'tomorrow', 'hate'), ('bunch', 'paki', 'perv'), ('chanyeol', 'achiev'), ('remind', 'prospect'), 'climb', ('make', 'femal'), 'yet', ('sheboon', 'hope'), ('complain', 'shed'), ('crazi', 'car'), ('leav', 'food', 'rudolph'), 'fab', ('like', '2', 'meet'), 'even', 'scratch', ('leav', 'countri'), ('europ', 'you'), 'p', 'believ', ('green', 'tomato'), ('doo', 'dog', 'bitch'), ('fuck', 'liber', 'fuck'), ('deal', 'understand'), ('citicar', 'job'), ('fuck', 'home', 'bye'), ('paleo', 'diet'), 'cba', ('appli', 'mostli', 'latino'), ('fuck', 'sand', 'monkey'), 'brexit', ('perv', 'clearli'), ('right', 'bear', 'arm'), ('ego', 'bit', 'girl'), ('rest', 'good', 'peopl'), '975', ('realiz', 'immigr'), ('fuck', 'hate', 'allah'), 'mosqu', ('europ', 'you', 'job'), ('fuck', 'polish'), ('come', 'home', 'off'), ('get', 'rid', 'them'), 'daughter', ('point', 'moron', 'trump'), ('baekhyun', 'work', 'ab'), 'retweet', ('one', 'dig'), 'seen', 'ye', ('see', 'firefli'), ('dog', 'bitch'), ('fuck', 'countri'), ('today', 'said', 'fuck'), ('bodi', '10'), ('difficult', 'deal'), ('fuck', 'paki', 'bastard'), ('work', 'out', 'chanyeol'), ('breastfeed', 'prematur', 'babi'), ('pay', 'you'), ('fuck', 'home'), ('round', 'good'), ('wanker', 'you', 'problem'), 'citizenship', ('wont', 'you'), 'look', 'download', ('trump', 'say'), 'mitch', ('bit', 'girl'), ('johnni', 'oti'), ('follow', 'paleo', 'diet'), ('rape', 'one', 'anim'), ('januari', 'go'), ('you', 'arriv'), ('gon', 'na', 'recolorshad'), ('fag', 'fagett'), ('caller', 'learn'), 'minut', ('awwir', 'rasslekast', 'eu'), ('go', 'preorder'), ('go', 'preorder', 'proud'), '50', 'hold', ('cool', 'off'), 'bye', ('margat', 'aug', '56'), ('funni', 'hold', 'shi'), ('danger', 'vegan'), ('common', 'decencyampcommon', 'sens'), ('stop', 'shout'), 'hour', 'better', ('even', 'though'), ('jar', 'fyld', 'coast'), ('never', 'tell', 'happi'), 'attent', ('get', 'visa', 'immigr'), 'leg', 'sho', ('stupid', 'ass', 'prick'), ('graffiti', 'train', 'today'), ('smoke', 'reggi', 'bush'), ('burn', 'hell', 'bitch'), 'tree', ('make', 'okay'), 'white', ('off', 'myriah', 'chavcentr'), 'fact', ('fuck', 'traitor', 'fuck'), 'veil', ('joke', 'you', 'go'), 'expand', ('lectur', 'end', 'januari'), ('fyld', 'coast'), 'sexist', 'religion', 'summer', 'poland', ('off', 'nail'), ('off', 'you'), ('girl', 'like'), ('new', 'home', 'today'), '07', 'whitepow', ('right', 'time', 'right'), ('0230', 'didnt'), ('jezani', 'cunt', 'cbb'), 'realli', ('see', 'ad', 'crisp'), ('mani', 'chink', 'london'), ('back', 'europ', 'you'), ('arm', 'kill'), ('custom', 'work'), ('get', 'you', 'out'), ('fuck', 'liber'), ('want', 'go'), ('fuck', 'racist'), ('face', 'behind'), ('off', 'xxx'), ('sock', 'give'), 'villa', ('everyth', 'you', 'get'), ('becom', 'physic', 'fit'), ('discount', 'believ'), 'oppo', 'preorder', 'world', ('peopl', 'cunt'), ('clevli', 'you', 'understand'), 'seek', ('sub', 'off'), ('ass', 'prick'), ('chang', 'smile', 'let'), ('get', 'back', 'featur'), 'bear', ('tame', 'bearwasnt'), ('meat', 'dairi'), 'happi', ('black', 'slagfound', 'pretti'), 'awwir', ('cool', 'off', 'amaz'), ('bother', 'cunt'), ('sub', 'ye'), ('from', 'leav', 'condition'), 'shithol', ('racist', 'sexist', 'perv'), ('awkward', 'moment', 'you'), ('shi', 'chest'), ('joshuawhyt', 'let'), ('petrolhead', 'fuck', 'treehugg'), ('you', 'think', 'wrong'), ('finish', 'roman', '117'), ('self', 'storag', 'facil'), ('thing', 'right'), ('polish', 'scum'), 'off', 'vs', ('off', 'new'), ('prematur', 'babi', 'boost'), 'ex', ('hate', 'peopl'), 'clearli', 'throne', 'accept', ('graffiti', 'train'), ('slagfound', 'pretti', 'funni'), ('shed', 'front', 'garden'), ('time', 'high'), ('polic', 'die'), ('look', 'fab', 'id'), ('break', 'smh'), ('strung', 'like'), ('leav', 'guy', 'home'), ('fuck', 'petrolhead'), ('preston', 'anymor', 'inconsider'), ('preston', 'anymor'), ('stupid', 'bitch'), ('look', 'fab'), ('get', 'readi'), ('father', 'christma', 'even'), ('five', 'hour'), ('sooo', 'mani'), ('need', 'daddi', 'wont'), ('take', 'advantag'), ('way', 'home', 'gon'), 'whore', 'spooki', 'joshua', ('pa', '321'), 'decid', ('off', 'discount'), ('fuck', 'joke', 'you'), ('whore', 'hope'), ('bitch', 'fuck'), 'thing', 'jezani', ('get', 'chelsea', 'ye'), ('realiz', 'immigr', 'fact'), 'back', ('egg', 'tourou'), ('kill', 'mani'), ('follow', 'topstori'), 'peopl', 'use', 'health', ('glad', 'difficult', 'deal'), ('love', 'you'), '128514', ('hasnt', 'kick', 'off'), ('extra', 'ankl', 'suppo'), 'school', 'said', ('mother', 'juandaearli'), ('mani', 'wog'), '20', 'truli', 'research', ('forget', 'robot'), ('wld', 'like'), ('record', 'onlin', 'emxzf'), ('dramat', 'sinc'), ('diy', 'terrorist', 'religion'), 'na', ('you', 'food'), ('fuck', 'traitor'), ('stay', 'around', 'truli'), ('off', 'outsid', 'leav'), ('go', 'au', 'natur'), ('last', 'time'), ('fagett', 'make', 'femal'), ('go', 'round'), ('allow', 'post', 'facebook'), ('fine', 'exampl', 'braveri'), ('fuck', 'metal'), 'crisp', ('least', 'nigger', 'lmfao'), ('scarlettjohansson', 'time'), ('terrorist', 'shit', 'diy'), 'polish', ('much', 'much'), 'home', ('iq', 'later', 'life'), ('local', 'paki', 'shop'), ('benjamin', 'fuck'), ('leav', 'guy'), ('men', 'sale', 'drop'), ('time', 'right', 'place'), ('even', 'allow'), ('box', 'tonight', 'great'), ('go', 'burn'), ('england', 'bunch', 'god'), 'rockit', ('night', 'fag'), ('irelandfew', 'yr'), 'differ', ('fuck', 'yourself', 'you'), 'hors', ('trythesewrestlingfanpodcast', 'awwir'), ('say', 'hell'), 'nearli', ('102', '10', 'off'), 'bunch', 'babi', ('fagett', 'femal', 'version'), 'extra', 'high', ('physic', 'fit'), ('readi', 'rumbl'), ('villa', 'countri'), ('give', 'citizenship', 'time'), 'probabl', ('take', 'immigr'), ('off', 'you', 'go'), ('daddi', 'wont', 'you'), ('cut', 'bitch'), ('off', 'road', 'run'), 'sinc', ('benjamin', 'fuck', 'faggot'), ('count', 'till', 'bryan'), 'soon', ('shredder', 'from'), ('cunt', 'wear'), ('recolorshad', 'head'), ('hand', 'like', 'poke'), ('britain', 'british'), ('fuck', 'off', 'home'), ('right', 'place'), 'johnni', ('make', 'right'), ('come', 'take'), ('like', 'fagett', 'make'), 'link', 'ab', 'sub', ('shut', 'them'), 'plan', ('back', 'poland'), ('off', 'mani', 'delici'), 'diet', ('point', 'out'), ('off', 'coast'), ('aug', '56', 'sponsor'), ('bastard', 'get', 'rid'), ('out', 'chanyeol'), ('you', 'nigger', 'sheboon'), ('get', 'chelsea'), ('natur', 'nail', 'violet'), ('boost', 'iq'), ('plan', 'never'), ('expand', '07', 'despit'), ('disgrac', 'moral', 'pygmi'), ('never', 'tell'), 'alien', 'terrorist', ('disgrac', 'moral'), ('beauti', 'place', 'anticip'), 'liber', 'natur', ('you', 'paki'), ('hate', 'nigger'), ('fuck', 'off', 'you'), 'worri', ('mani', 'chink'), ('though', 'want', 'go'), 'but1', ('2', 'meet', 'j'), 'broke', 'sit', ('paki', 'peopl'), 'spicskkk', 'palm', ('hate', 'even'), ('50', 'get', 'back'), 'want', 'fear', ('ex', 'broke', 'vs'), 'lmfao', ('say', 'face'), ('respect', 'uncoupl', 'record'), ('39', 'day', 'go'), 'unaccept', ('christma', 'even'), ('myriah', 'chavcentr', 'chav'), ('chutney', 'vintag', '2'), ('leav', 'countri', 'brexit'), ('fuck', 'treehugg', 'steve'), 'scarlett', ('liber', 'fuck'), ('guy', 'home'), ('around', 'scream'), ('job', 'careerarc'), ('appli', 'mostli'), ('storag', 'facil'), ('meet', 'j', 'bloer'), 'fat', 'wont', ('promis', 'p', 'dude'), ('ab', 'gotten'), ('sooo', 'mani', 'spooki'), ('jacksonvil', 'fl'), ('spain', 'economi'), ('stay', 'around'), ('nowplay', 'sun', 'lancast'), 'coast', 'fucker', 'realiz', ('bit', 'girl', 'like'), ('bunch', 'paki'), ('dairi', 'egg'), ('joshua', 'forget', 'robot'), 'poignantli', ('learn', 'hate', 'polit'), ('kick', 'off'), 'kill', ('last', 'weekend'), ('terrorist', 'religion', 'fuck'), ('economi', 'expand', '07'), 'offer', 'leagu', ('crisp', 'cover', 'gold'), ('god', '128514'), 'yall', ('snap', 'tule'), ('let', 'terrorist', 'out'), 'xxx', 'food', ('scoobi', 'doo'), ('discount', 'believ', 'mani'), ('differ', 'hors'), ('rasslekast', 'eu'), ('kill', 'mani', 'peopl'), 'american', ('one', 'anim'), ('ago', 'looool'), 'front', 'pleas', ('black', 'bitch', 'next'), ('cinema', 'next'), 'chav', ('babi', 'boost', 'iq'), ('sexist', 'perv'), ('five', 'hour', 'kill'), 'treehugg', ('countri', 'you'), ('offer', 'oppoun', 'pay'), ('hate', 'allah'), ('problem', 'say', 'face'), ('bye', 'bye'), ('call', 'black'), ('fuck', 'fascist', 'fuck'), ('whore', 'hope', 'get'), ('broke', 'vs'), 'right', '10', 'complet', ('aug', '56'), ('version', 'fag'), 'deadlock', ('better', 'stop', 'shout'), ('here', 'oppo', 'good'), ('sun', 'lancast'), 'breastfeed', 'show', ('home', 'oreo', 'drunk'), 'forget', ('bastard', 'muslim', 'dog'), ('hous', 'hotel'), ('test', 'advic'), ('work', 'citi', 'hire'), ('moral', 'pygmi'), ('crisp', 'cover'), ('hasnt', 'kick'), 'still', ('scarlettjohansson', 'time', 'test'), ('bitch', 'road'), ('nigger', 'hate'), ('chang', 'smile'), ('from', 'rockit', 'round'), ('fuck', 'annoy'), ('correctnessbut', 'love', 'common'), ('nigger', 'road'), ('fear', 'polic', 'die'), ('nearli', 'much', 'you'), 'make', 'chink', ('50', 'get'), ('let', 'smile'), ('fuck', 'off', 'back'), 'countri', ('end', 'nigger'), ('39', 'day'), ('today', 'realiti'), 'au', ('shithol', 'go'), ('mani', 'delici', 'treat'), ('went', 'off', '0230'), ('metal', 'imagin', 'train'), ('deserv', 'everyth', 'you'), ('hand', 'nuala', 'nowplay'), ('least', 'look', 'like'), ('morn', 'alarm'), ('love', 'someon', 'tell'), ('right', 'bear'), ('kill', 'kick'), ('bk', 'tri', 'give'), 'end', ('even', 'countri'), 'mother', ('neighbour', 'complain', 'shed'), ('id', 'polish'), ('heaoverh', 'kenchan', 'onbsc2016'), ('one', 'ok'), ('pygmi', 'paul', 'ryan'), 'yourself', 'write', 'anymor', ('jefre', 'starr', 'faggot'), ('foreign', 'bastard'), ('thing', 'right', 'time'), ('off', 'back', 'europ'), ('fuck', 'terrorist'), ('muslim', 'scum'), ('ugli', 'but1'), ('allow', 'post'), ('rasslekast', 'eu', 'sat'), 'cours', ('nigger', 'leg', '2'), 'appli', ('today', 'tame'), 'mamp', ('fuck', 'you', 'nigger'), 'kinda', 'rhose', ('chang', 'tune'), ('coast', 'irelandfew'), ('natur', 'nail'), ('never', 'happen'), ('stop', 'shout', 'go'), ('scottish', 'bastard', 'get'), 'rasslekast', ('get', 'rape'), ('detail', 'job', 'citicar'), ('polish', 'alien'), ('everywher', 'get', 'rid'), ('game', 'throne', 'univers'), 'littl', ('off', 'you', 'scum'), 'rain', ('you', 'back', 'paki'), ('sun', 'lancast', 'download'), ('alarm', 'went'), ('world', 'chang'), 'vermin', 'trythesewrestlingfanpodcast', ('you', 'understand'), ('shut', 'fuck'), 'crep', 'hotel', ('love', 'someon'), ('round', 'shut', 'them'), 'didnt', ('figur', 'shit'), ('day', 'make', 'britain'), ('second', 'ago', 'looool'), ('fab', 'id', 'polish'), 'petrolhead', ('egg', 'get', 'well'), ('immigr', 'poignantli'), ('day', 'make'), ('imagin', 'train', 'prepar'), ('come', 'take', 'job'), ('never', 'turn', 'back'), 'robot', ('gold', 'glitteri', 'star'), 'awkward', 'new', ('prospect', 'arent', 'woh'), ('fit', 'scarlett'), ('dramat', 'sinc', 'simon'), ('breastfeed', 'prematur'), 'benjamin', ('citi', 'hire'), ('decid', 'go', 'au'), ('back', 'europ'), 'drag', ('countri', 'brexit'), ('jar', 'fyld'), 'facil', 'cover', ('click', 'detail'), ('muslim', 'terrorist', 'same'), ('fuck', 'racist', 'fuck'), 'ass', ('economi', 'expand'), ('garden', 'who', 'neighbour'), 'onbsc2016', ('preorder', 'proud'), ('get', 'you', 'link'), ('hate', 'allah', 'lover'), 'wanker', ('go', 'go'), ('road', 'better'), ('great', 'fight', 'joshuawhyt'), ('you', 'accept'), ('strung', 'like', 'nigger'), ('inconsider', 'twat', 'm6m61'), ('decid', 'go'), ('polish', 'custom', 'work'), 'faggot', 'someon', 'shredder', ('gon', 'na'), ('someon', 'stay', 'around'), ('you', 'job', 'steal'), ('myriah', 'chavcentr'), ('nigger', 'leg'), ('kondobyjaymoni', '20', 'brazil'), 'pork', ('like', 'nigger', 'whitepow'), ('common', 'decencyampcommon'), 'love', '117', ('buddi', 'resid'), 'merri', 'prepar', ('pork', 'scratch'), 'pretti', ('keep', 'see'), ('immigr', 'work'), ('meat', 'dairi', 'egg'), ('peopl', 'you', 'usconstitut'), 'head', ('orangi', 'countri'), ('like', 'nigger'), ('orangi', 'countri', 'sub'), ('vintag', '2'), ('off', 'outsid'), ('crazi', 'car', 'crash'), ('a4b', '36'), ('coast', 'irelandfew', 'yr'), ('even', 'allow', 'countri'), ('caller', 'learn', 'hate'), ('you', 'black', 'bastard'), ('kick', 'off', 'mani'), ('hors', 'beat', 'hors'), ('sta', 'finish'), ('health', 'famili', 'breastfeed'), ('dairi', 'egg', 'tourou'), ('you', 'link'), ('go', 'back', 'sleep'), ('hold', 'shi'), ('glitteri', 'star', 'you'), ('men', 'sale'), 'outsid', ('muslim', 'terrorist', 'rest'), ('shithol', 'go', 'x'), 'year', ('much', 'you'), ('finger', 'snap', 'tule'), 'job', 'keep', ('go', 'around', 'scream'), ('spooki', 'spider'), ('enough', 'fuck'), ('poke', 'finger', 'snap'), ('word', 'end'), ('fit', 'scarlett', 'johansson'), ('get', 'jar'), ('a4b', '36', 'hoursdishoomincinema'), ('violet', '102'), ('you', 'immigr'), ('get', 'readi', 'rumbl'), ('clevli', 'you'), 'see', ('you', 'steph'), ('scratch', 'minut'), ('10', 'off', 'discount'), ('off', 'immigr'), ('exampl', 'braveri'), ('you', 'arriv', 'town'), 'sooo', ('firefli', 'squid', 'japan'), ('faggot', 'hate', 'spicskkk'), ('nigger', 'road', 'better'), ('treehugg', 'steve'), 'christma', ('krtk', 'baskn'), ('polish', 'bitch'), 'oreo', ('slagfound', 'pretti'), ('see', 'firefli', 'squid'), 'wayn', ('someon', 'tell', 'them'), 'oran', 'lol', ('moral', 'pygmi', 'paul'), ('shed', 'front'), ('you', '50', 'get'), ('fuck', 'hate', 'you'), ('hate', 'leav'), ('trump', 'say', 'hell'), ('count', 'till'), 'sale', ('updat', 'databas'), ('bad', '1', 'muslim'), 'good', ('from', 'sta'), 'polit', ('black', 'bastard', 'go'), 'chutney', 'twat', 'uncoupl', ('get', 'enough'), ('back', 'paki'), ('sale', 'drop'), ('you', 'steph', 'mate'), ('out', 'hand'), ('prematur', 'babi'), 'email', ('tree', 'bird'), ('shit', 'diy', 'faggot'), ('treehugg', 'steve', 'you'), 'god', 'diy', ('father', 'christma'), ('keep', 'britain'), ('let', 'world', 'chang'), ('kondobyjaymoni', '20'), ('star', 'you', 'get'), ('you', 'never', 'tell'), ('fight', 'joshuawhyt'), ('sight', 'accomplish'), ('morn', 'alarm', 'went'), ('sheboon', 'hope', 'you'), ('kenchan', 'onbsc2016'), ('alien', 'come', 'take'), ('woh', 'nearli'), ('deserv', 'everyth'), ('fact', 'work'), ('sale', 'drop', 'dramat'), ('peopl', 'cunt', 'much'), ('mostli', 'latino'), ('hire', 'jacksonvil', 'fl'), ('hotel', 'wrea'), ('fuck', 'sand'), ('fuck', 'fascist'), ('much', 'you', 'think'), ('immigr', 'work', 'golf'), '1488', ('let', 'world'), 'scoobi', 'bother', ('off', 'you', 'fuck'), 'scottish', ('difficult', 'deal', 'understand'), ('diet', 'danger'), ('you', 'polish', 'ego'), ('high', 'immigr'), ('woh', 'nearli', 'much'), ('hors', 'beat'), ('right', 'place', 'long'), 'breaktim', ('you', 'think'), 'sleep', 'smelli', ('amaz', 'product'), 'mate', ('shop', 'disgust', 'even'), 'akbar', ('shi', 'chest', 'seen'), ('baskn', 'oran', 'akp'), ('glad', 'difficult'), 'immigr', 'lost', 'wrong', 'finish', ('oran', 'akp'), 'common', 'car', 'word', ('off', '0230', 'didnt'), ('decid', 'fuck', 'off'), ('email', 'get'), ('car', 'crash'), ('but1', 'text'), ('cake', 'sale', 'kick'), ('you', 'muslim'), ('yr', 'bk', 'tri'), ('attent', 'seek', 'bitch'), ('you', 'love'), ('outreach', 'email'), ('bye', 'bye', 'brexit'), ('achrostmad', 'villa', 'countri'), ('but1', 'text', 'back'), ('femal', 'hate'), 'elderrumb', ('brexit', 'time'), ('later', 'life'), ('enjoy', 'box'), ('hour', 'go'), ('ad', 'crisp'), ('immigr', 'brexit'), ('bearwasnt', 'hard'), 'went', ('cba', 'polish'), ('fuck', 'daughter'), ('go', 'au'), 'topstori', ('rhose', 'word', 'end'), ('girl', 'like', 'think'), 'traitor', ('off', 'back', 'poland'), ('margat', 'aug'), ('paki', 'fuck', 'off'), ('make', 'right', 'sight'), ('hate', 'spicskkk'), ('kill', 'kick', 'off'), 'disgust', 'onlin', ('american', 'give'), ('you', 'fuck', 'faggot'), 'dig', 'polic', ('hope', 'get'), ('spread', 'leg'), ('back', 'shithol'), 'tie', 'work', 'crash', ('go', 'lectur', 'end'), ('know', 'someon', 'stay'), ('off', '0230'), 'featur', 'say', 'cakepop', ('anim', 'might', 'chang'), ('job', 'steal'), ('merri', 'achrostmad', 'villa'), ('terrorist', 'religion'), ('you', '50'), ('watch', 'season'), ('boost', 'iq', 'later'), 'gotten', ('way', 'home'), 'go', ('front', 'garden', 'who'), 'figur', ('leg', '2', 'differ'), ('behind', 'back', 'faggot'), 'cbb', ('show', 'nowplay', 'sun'), ('reggi', 'bush'), ('countri', 'sub'), ('polish', 'ego', 'bit'), ('hate', 'faggot'), ('fuck', 'yourself'), ('love', 'common'), 'proud', ('home', 'gon', 'na'), ('use', 'tie'), ('correctnessbut', 'love'), 'europ', ('3', 'ill'), ('you', 'go', 'around'), ('out', 'disgrac'), ('differ', 'hors', 'beat'), ('tomato', 'chutney', 'vintag'), ('ye', 'sub'), ('give', 'extra', 'ankl'), ('sli', 'bitch'), ('go', 'round', 'shut'), ('go', 'home', 'you'), ('nuala', 'nowplay'), ('hate', 'yellow'), 'long', 'local', ('mosqu', 'let'), ('yet', 'get', 'visa'), 'xfactor', ('fine', 'tomorrow'), ('cakepop', 'out'), ('work', 'ab', 'gotten'), 'season', ('paleo', 'diet', 'danger'), ('you', 'get', 'them'), ('off', 'pleas'), ('10', 'off'), ('lectur', 'end'), ('fact', 'bodi', '10'), ('yourself', 'you'), 'myriah', ('said', '975'), ('funni', 'hold'), ('click', 'detail', 'job'), ('today', 'ribchest'), 'xxxx', ('local', 'mosqu', 'let'), ('neighbour', 'call'), 'central', ('season', 'two', 'internet'), ('out', 'disgrac', 'moral'), ('gon', 'na', 'cut'), ('health', 'famili'), ('hate', 'faggot', 'like'), ('paki', 'bastard'), 'paleo', ('sinc', 'simon'), ('use', 'tie', 'end'), ('off', 'discount', 'believ'), 'margat', ('even', 'though', 'want'), ('beat', 'hors'), ('au', 'natur', 'xfactor'), ('internet', 'decid', 'fuck'), ('tonight', 'great', 'fight'), ('chavcentr', 'chav', 'central'), 'much', ('countri', 'sub', 'ye'), ('featur', 'lost'), 'internet', ('fuck', 'petrolhead', 'fuck'), ('world', 'heaoverh', 'kenchan'), ('one', 'hour'), ('ryan', 'mitch'), ('cover', 'gold', 'glitteri'), ('you', 'fuck', 'irish'), ('leav', 'condition', 'yet'), ('probabl', 'gon', 'na'), ('religion', 'fuck', 'joke'), 'yr', ('believ', 'mani'), 'tame', 'box', ('tule', 'tank'), 'church', ('good', 'egg'), ('amaz', 'product', 'from'), 'winckley', 'vegan', ('keep', 'britain', 'british'), ('didnt', 'know'), ('hold', 'let'), 'tire', ('cba', 'polish', 'custom'), ('paki', 'fuck'), ('church', 'england', 'bunch'), 'facebook', ('annoy', 'meat'), ('tri', 'climb'), ('ego', 'bit'), ('bodi', '10', 'make'), 'lancast', 'everyth', ('peopl', 'allow', 'post'), 'smh', ('peopl', 'respect'), 'muslim', ('traitor', 'fuck'), 'krtk', 'hate', ('retweet', 'gazet', 'krtk'), ('starr', 'faggot'), 'chanyeol', ('whitepow', '1488'), ('understand', 'you'), 'shop', ('them', 'thing', 'right'), ('till', 'bryan', 'new'), ('moron', 'trump', 'say'), ('train', 'today'), 'ago', 'ok', ('off', 'home'), ('road', 'annoy'), ('fuck', 'irish'), ('visa', 'immigr'), ('thankyou', 'you'), ('wog', 'road'), 'wog', ('scare', 'father'), ('you', 'strung', 'like'), 'run', ('from', 'leav'), 'rid', ('compil', 'pa'), ('kinda', 'glad'), 'day', 'fascist', ('citicar', 'job', 'careerarc'), ('you', 'cunt'), 'lectur', ('gotten', 'realli', 'skinni'), 'get', 'firefli', 'hoursdishoomincinema', 'meme', ('who', 'neighbour', 'church'), 'steph', ('terror', 'bad', '1'), 'fag', ('bryan', 'new', 'ep'), 'fine', ('finish', 'roman'), ('test', 'advic', 'becom'), ('god', 'make', 'right'), ('time', 'fuck'), ('drop', 'dramat'), ('ankl', 'suppo'), ('fagett', 'femal'), ('sit', 'cinema'), ('want', 'work'), ('steph', 'mate'), ('diy', 'paki'), '39', ('merri', 'achrostmad'), 'understand', ('wld', 'like', '2'), ('peopl', 'allow'), ('round', 'good', 'egg'), ('take', 'advantag', '10'), ('sta', 'finish', 'roman'), ('weekend', 'take', 'advantag'), ('give', 'citizenship'), ('respect', 'uncoupl'), ('said', 'fuck', 'off'), 'maniq', ('go', 'back'), ('tomorrow', 'hate'), ('word', 'end', 'you'), ('think', 'wrong'), 'graffiti', ('fag', 'like'), ('sand', 'monkey'), 'suppo', 'sight', ('off', 'worri'), ('decis', 'leader', 'hold'), 'eu', ('like', 'jefre', 'starr'), 'difficult', 'mani', 'becom', ('hard', 'you'), ('daddi', 'come', 'make'), ('lilli', 'tri', 'climb'), ('nigger', 'everywher', 'day'), ('hell', 'bitch'), ('least', 'nigger'), 'round', ('soon', 'wayn', 'x'), ('975', 'peopl'), 'kick', ('polit', 'correctnessbut'), ('bastard', 'muslim'), ('nigger', 'sheboon'), ('understand', 'you', 'deserv'), ('well', 'soon', 'wayn'), ('later', 'life', 'research'), ('bloer', 'cafe', 'clevli'), ('turn', 'back', 'bird'), ('fuck', 'leagu', 'meme'), ('condition', 'yet'), 'meat', ('post', 'fine', 'exampl'), ('rockit', 'round'), ('2', 'differ'), 'dyke', 'tri', ('daddi', 'need'), ('oreo', 'drunk'), ('black', 'countri'), ('fuck', 'anyon'), ('compani', 'still', 'hasnt'), ('palm', 'hand', 'like'), ('topstori', 'spain', 'economi'), 'crazi', ('doo', 'dog'), ('yall', 'proud', 'baekhyun'), ('paki', 'cunt', 'wear'), ('school', 'sta'), ('go', 'home'), 'updat', 'tank', ('fact', 'bodi'), 'london', ('hour', 'kill', 'kick'), ('proud', 'baekhyun', 'work'), ('spain', 'economi', 'expand'), ('readi', 'rumbl', 'joshua'), ('moment', 'you', 'arriv'), ('who', 'neighbour'), ('iq', 'later'), ('fucker', 'leav', 'countri'), 'higher', 'condition', ('take', 'job'), ('job', 'citicar', 'job'), 'nowplay', ('jezani', 'cunt'), 'mostli', ('tell', 'god', 'make'), ('next', 'shut'), ('fat', 'littl', 'wanker'), ('ankl', 'suppo', 'off'), ('januari', 'go', 'drag'), ('famili', 'breastfeed', 'prematur'), 'egg', ('off', 'amaz', 'product'), ('j', 'bloer', 'cafe'), 'shed', ('2', 'meet'), ('neighbour', 'call', 'paki'), ('immigr', 'poignantli', 'point'), ('work', 'ab'), ('ugli', 'but1', 'text'), ('hate', 'spicskkk', 'ralli'), 'ribchest', ('wog', 'leav'), ('violet', '102', '10'), 'hous', ('make', 'femal', 'hate'), ('oppoun', 'pay', 'you'), ('beat', 'hors', 'spread'), ('proud', 'baekhyun'), 'danger', ('nigger', 'whitepow', '1488'), ('from', 'rockit'), ('you', 'out'), ('better', 'stop'), ('home', 'you'), 'baekhyun', ('green', 'tomato', 'chutney'), 'gazet', ('off', 'mani'), 'treat', ('maniq', 'gel', 'polish'), ('one', 'hour', 'go'), ('write', 'margat', 'aug'), ('hate', 'nigger', 'hate'), '15', ('call', 'paki', 'peopl'), ('ex', 'broke'), ('work', 'yet'), ('black', 'bitch'), 'rumbl', 'sheboon', 'sand', ('polish', 'natur', 'nail'), 'news', ('road', 'run'), 'gon', ('job', 'unaccept'), ('fuck', 'chink'), ('make', 'okay', 'justwond'), ('chanyeol', 'achiev', 'give'), 'die', 'tomato', ('hand', 'like'), ('leav', 'condition'), 'de', 'stay', ('faggot', 'like'), ('baskn', 'oran'), ('facebook', '15', 'reason'), ('cakepop', 'out', 'off'), ('tomorrow', 'love', 'you'), ('love', 'follow', 'paleo'), ('sub', 'off', 'coast'), ('fagett', 'make'), ('countri', 'girl', 'johnni'), ('alien', 'come'), ('ok', 'rock'), ('anim', 'might'), 'metal', ('train', 'today', 'said'), ('head', 'crep'), ('home', 'bye', 'bye'), 'from', 'scare', 'night', 'cleveleysno', 'need', ('beauti', 'place'), ('local', 'mosqu'), ('get', 'well'), 'promis', ('version', 'fag', 'like'), 'decencyampcommon', ('wear', 'veil'), 'tomorrow', 'realiti', ('need', 'daddi'), ('wrong', 'said'), 'burn', ('faggot', 'hate'), ('diy', 'faggot'), ('strip', 'second'), 'here', ('you', 'even'), ('cunt', 'cbb'), 'last', ('text', 'back', '3sec2'), ('allah', 'lover'), ('racist', 'sexist'), ('cafe', 'clevli', 'you'), 'tourou', ('m6m61', 'way', 'home'), ('let', 'get'), ('mani', 'peopl'), ('let', 'get', 'readi'), ('happi', 'bihday'), ('bastard', 'go'), ('tire', 'polish', 'bitch'), ('polish', 'off', 'worri'), ('chavcentr', 'chav'), ('work', 'citi'), ('chink', 'london'), ('go', 'burn', 'hell'), 'moron', ('2', 'year'), ('you', 'black', 'slagfound'), ('life', 'research'), ('train', 'prepar', 'one'), ('good', 'buddi'), ('terrorist', 'same'), 'jar', 'simon', ('fuck', 'go', 'x'), ('retweet', 'gazet'), ('twat', 'm6m61'), 'moment', ('sale', 'kick'), ('polish', 'bastard'), 'thankyou', 'bird', '3sec2', 'dog', ('man', 'call', 'black'), ('burn', 'hell'), 'chavcentr', ('come', 'out'), 'prematur', 'drop', ('daughter', 'disgust'), ('you', 'out', 'hand'), ('life', 'research', 'find'), ('peopl', 'wrong'), 'custom', ('3', 'ill', 'buy'), ('metal', 'imagin'), ('throne', 'univers'), ('hate', 'faggot', 'hate'), ('fulli', 'them', 'palm'), ('arent', 'woh', 'nearli'), ('20', 'brazil', 'beauti'), ('bastard', 'go', 'back'), 'smoke', 'give', ('white', 'bitch'), 'cunt', ('okay', 'justwond'), ('chutney', 'vintag'), ('yet', 'get'), ('oti', 'show', 'nowplay'), ('joshuawhyt', 'let', 'get'), ('fuck', 'hate'), ('nowplay', 'sun'), 'ego', ('natur', 'xfactor'), 'iq', ('fyld', 'coast', 'green'), ('lol', 'fear', 'polic'), ('everywher', 'get'), 'though', 'gold', ('immigr', 'fuck'), ('record', 'onlin'), ('off', 'avfc'), ('immigr', 'keep'), 'sens', 'buy', ('home', 'off', 'xxx'), 'sponsor', ('you', 'black'), ('joshua', 'forget'), ('custom', 'work', 'like'), '321', ('bitch', 'next', 'shut'), ('front', 'garden'), ('prospect', 'arent'), '36', ('topstori', 'spain'), ('fl', 'click'), ('saw', 'graffiti', 'train'), ('you', 'scum'), ('time', 'fuck', 'off'), ('fear', 'polic'), ('irelandfew', 'yr', 'bk'), ('climb', 'tree', 'bird'), 'happen', ('disgust', 'even', 'allow'), ('simon', 'decid', 'go'), 'brazil', ('petrolhead', 'fuck'), ('church', 'england'), 'two', 'exampl', ('good', 'night', 'fag'), ('daddi', 'daddi', 'come'), 'road', ('faggot', 'sit', 'cinema'), ('3sec2', 'funni', '3'), ('get', 'rape', 'one'), ('tire', 'polish'), ('till', 'bryan'), ('end', 'januari'), 'them', ('someon', 'stay'), ('thank', 'you', 'post'), ('one', 'anim', 'might'), ('1', 'muslim', 'terrorist'), 'glad', ('polish', 'natur'), ('thick', 'scottish', 'bastard'), 'beat', ('complet', 'fail'), 'neighbour', ('wog', 'road', 'annoy'), ('place', 'long'), 'ralli', ('bird', 'complet', 'fail'), ('get', 'you'), ('ill', 'buy'), ('like', 'think', 'ok'), ('fl', 'click', 'detail'), ('like', 'fagett'), ('cheat', 'fuck', 'stupid'), ('internet', 'decid'), ('despit', 'polit'), ('know', 'fuck'), ('arent', 'woh'), 'starr', ('time', 'preston'), ('buddi', 'resid', 'shredder'), ('allow', 'countri'), ('bearwasnt', 'hard', 'you'), ('get', 'jar', 'fyld'), 'doo', ('love', 'common', 'decencyampcommon'), ('terrorist', 'rest', 'good'), ('tell', 'happi', 'bihday'), ('10', 'off', 'nail'), ('crash', 'compil'), ('diy', 'terrorist'), ('fuck', 'leagu'), ('anim', 'id', 'love'), 'steve', 'spread', ('polish', 'alien', 'come'), ('get', 'well', 'soon'), ('braveri', 'inspir'), 'call', ('today', 'tame', 'bearwasnt'), ('fuck', 'nigger'), 'shout', ('get', 'them'), ('out', 'off', 'new'), ('prepar', 'one', 'dig'), ('work', 'golf'), ('them', 'palm'), ('ep', '39'), 'careerarc', 'gel', ('hate', 'immigr', 'fuck'), ('you', 'sock'), 'prick', 'bearwasnt', ('chang', 'world', 'heaoverh'), ('god', 'bother'), ('technolog', 'decis', 'leader'), ('tell', 'happi'), ('go', 'drag', 'freedom'), 'enough', ('nigger', 'whore', 'hope'), ('black', 'countri', 'man'), ('fuck', 'metal', 'imagin'), 'snap', ('clearli', 'want'), ('stupid', 'ass'), ('fuck', 'faggot', 'sit'), ('neighbour', 'complain'), ('sho', 'outreach', 'email'), ('you', 'you', 'paki'), ('cunt', 'fuck', 'off'), ('though', 'want'), 'strip', 'might', ('figur', 'shit', 'out'), 'fuck', 'inspir', ('mani', 'wog', 'leav'), ('achrostmad', 'villa'), 'pay', ('fuck', 'you'), ('fine', 'tomorrow', 'love'), ('write', 'margat'), 'hope', 'man', ('fuck', 'foreign'), ('hate', 'you'), ('daddi', 'need', 'daddi'), ('facebook', '15'), ('you', 'go', 'christma'), ('leader', 'hold', 'let'), 'buddi', 'oti', 'fl', ('callin', 'daddi'), 'yellow', ('muslim', 'dog'), ('citizenship', 'time', 'high'), ('you', 'daddi', 'daddi'), ('simon', 'decid'), 'readi', ('palm', 'hand'), 'glitteri', ('1', 'muslim'), ('end', 'nigger', 'leg'), ('end', 'you', 'think'), ('rid', 'them', 'bastard'), ('truli', 'want'), 'clevli', ('time', 'high', 'immigr'), 'come', ('bunch', 'god'), 'hell', ('back', 'sleep'), 'father', ('gold', 'glitteri'), ('fuck', 'jezani'), '0230', ('soon', 'wayn'), ('you', 'job'), ('immigr', 'fuck', 'anyon'), ('daddi', 'daddi', 'need'), ('joke', 'you'), ('maniq', 'gel'), ('rudolph', 'thankyou'), ('fuck', 'joke'), ('go', 'around'), 'wrea', ('wrea', 'green'), ('like', 'realiz', 'immigr'), 'rock', ('well', 'last', 'weekend'), ('physic', 'fit', 'scarlett'), ('id', 'love', 'follow'), ('polish', 'custom'), ('hour', 'kill'), '1', ('scum', 'game', 'throne'), 'hire', ('you', 'foreign', 'bastard'), ('hate', 'polit'), ('countri', 'you', 'fuck'), 'despit', 'wld', ('bunch', 'god', 'bother'), ('awkward', 'moment'), ('hire', 'jacksonvil'), 'around', 'next', 'famili', ('kick', 'off', 'avfc'), 'dramat', ('man', 'time'), 'bihday', 'select', ('climb', 'tree'), ('oh', 'god'), ('say', 'hell', 'put'), ('leader', 'hold'), ('from', 'sta', 'finish'), 'tule', ('them', 'along'), 'shit', 'tune', ('home', 'you', 'black'), ('off', 'you', 'polish'), ('07', 'despit'), ('them', 'thing'), ('back', 'countri', 'rude'), 'cafe', 'latino', ('expand', '07'), ('j', 'bloer'), ('villa', 'countri', 'hous'), ('terrorist', 'everywher'), ('home', 'oreo'), 'bruh', ('them', 'mamp', 'cleveleysno'), 'visa', ('last', 'weekend', 'take'), 'depo', 'recolorshad', ('meme', 'hilari')}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at how frequent each feature is in the dataset. (_If you can't see the graph, try running the code chunk again_).\n",
    "\n",
    "Recall that features are tokens (unigrams) or combination of tokens (ngrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11e11fb38>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = nltk.FreqDist(list_features)\n",
    "fd.plot(20, cumulative=True)\n",
    "#fd.xlabel('Most common features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fuck', 57), ('you', 53), ('off', 30), ('go', 19), ('get', 17), (('fuck', 'off'), 16), ('hate', 16), ('paki', 15), ('bastard', 14), ('bitch', 13), ('immigr', 13), ('nigger', 12), ('countri', 11), ('polish', 11), ('back', 10), ('cunt', 10), ('out', 10), ('home', 9), ('like', 9), ('terrorist', 9), ('muslim', 8), ('faggot', 8), (('off', 'you'), 7), ('them', 7), ('time', 7), ('leav', 7), ('right', 7), ('black', 6), (('fuck', 'off', 'you'), 6), ('mani', 6), ('peopl', 6), ('work', 6), ('make', 5), ('let', 5), ('good', 5), ('love', 5), ('daddi', 5), (('fuck', 'paki'), 4), ('come', 4), ('job', 4), ('even', 4), ('brexit', 4), ('road', 4), ('shut', 4), ('scum', 4), ('want', 4), ('today', 4), (('you', 'fuck'), 4), ('one', 4), ('tell', 4)]\n"
     ]
    }
   ],
   "source": [
    "print(fd.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to select a sample of this vocabulary of features. We only want to keep the features that truely matter in identifying hate speech.\n",
    "\n",
    "This step is important to reduce the  running time of our model as well as improve its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping the 50 most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rare features are only present in one or two tweet. We know that these are not going to be very useful to teach the model to recognise hate speech.  \n",
    "\n",
    "Let's only keep the top 50 most frequent features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chosen features: 50/2443\n"
     ]
    }
   ],
   "source": [
    "def get_word_features(wordlist, n):\n",
    "    fd = nltk.FreqDist(wordlist)\n",
    "    \n",
    "    word_features = sorted(fd.items(), key=operator.itemgetter(1), reverse=True)[0:n] \n",
    "    word_features = [i[0] for i in word_features ]\n",
    "    return word_features\n",
    "\n",
    "# Only keep the top 50 most frequent words\n",
    "chosen_features = get_word_features(list_features, 50)\n",
    "print('Number of chosen features: {}/{}'.format(len(chosen_features), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fuck', 'you', 'off', 'go', 'get', ('fuck', 'off'), 'hate', 'paki', 'bastard', 'bitch', 'immigr', 'nigger', 'countri', 'polish', 'back', 'cunt', 'out', 'home', 'like', 'terrorist', 'muslim', 'faggot', ('off', 'you'), 'them', 'time', 'leav', 'right', 'black', ('fuck', 'off', 'you'), 'mani', 'peopl', 'work', 'make', 'let', 'good', 'love', 'daddi', ('fuck', 'paki'), 'come', 'job', 'even', 'brexit', 'road', 'shut', 'scum', 'want', 'today', ('you', 'fuck'), 'one', 'tell']\n"
     ]
    }
   ],
   "source": [
    "print(chosen_features[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input data for classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have chosen a sample of features that we think are important for the model to learn to identify hateful speech.\n",
    "\n",
    "However, at this stage the classfier won't be able to know which features are responsible for a tweet being labelled as 'hateful'. Is it because of the word 'road' or the word 'bastard' in that tweet?\n",
    "\n",
    "To be able to learn what counts as hateful and what doesn't, the classifier needs to know the 'hateful value' of each feature in the vocabulary.\n",
    "\n",
    "In short, we need to tell the model:\n",
    "- which features are typically present in hateful tweets and which are not,\n",
    "- which features are typically present in non-hateful tweets and which are not.\n",
    "\n",
    "This precious information is available in our dataset because it has been manually labelled. So far we have not used the 'class' column in our dataset. We are now going to make use of it!\n",
    "\n",
    "The idea is to tell the model: \n",
    "- for each hateful tweet: these are the features present, and the ones not present. \n",
    "- for each non-hateful tweet: these are the features present, and the ones not present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the features present in each tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    feature_set = {}\n",
    "    for feature in chosen_features:\n",
    "        feature_set['contains({})'.format(feature)] = (feature in document_words)\n",
    "    return feature_set\n",
    "\n",
    "tweets = [tuple(x) for x in pre_processed_data.values]\n",
    "\n",
    "feature_set = nltk.classify.apply_features(extract_features, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in training_set: 199\n"
     ]
    }
   ],
   "source": [
    "print('Number of tweets in training_set: {}'.format(len(feature_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets' look at the first tweet. Notice at the end, we see that this is a hateful tweet (label = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'contains(fuck)': False, 'contains(you)': False, 'contains(off)': False, 'contains(go)': True, 'contains(get)': False, \"contains(('fuck', 'off'))\": False, 'contains(hate)': False, 'contains(paki)': False, 'contains(bastard)': True, 'contains(bitch)': False, 'contains(immigr)': False, 'contains(nigger)': False, 'contains(countri)': True, 'contains(polish)': False, 'contains(back)': True, 'contains(cunt)': False, 'contains(out)': False, 'contains(home)': False, 'contains(like)': False, 'contains(terrorist)': False, 'contains(muslim)': False, 'contains(faggot)': False, \"contains(('off', 'you'))\": False, 'contains(them)': False, 'contains(time)': False, 'contains(leav)': False, 'contains(right)': False, 'contains(black)': True, \"contains(('fuck', 'off', 'you'))\": False, 'contains(mani)': False, 'contains(peopl)': False, 'contains(work)': False, 'contains(make)': False, 'contains(let)': False, 'contains(good)': False, 'contains(love)': False, 'contains(daddi)': False, \"contains(('fuck', 'paki'))\": False, 'contains(come)': False, 'contains(job)': False, 'contains(even)': False, 'contains(brexit)': False, 'contains(road)': False, 'contains(shut)': False, 'contains(scum)': False, 'contains(want)': False, 'contains(today)': False, \"contains(('you', 'fuck'))\": False, 'contains(one)': False, 'contains(tell)': False}, 1)\n"
     ]
    }
   ],
   "source": [
    "print(feature_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method is pretty simple. For each tweet, we are looping through our 50 chosen_features and setting a boolean to True if the tweet contains that feature, False otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is what the classifier needs as input. It is a series of 0s and 1s (numerical) as opposed to text data that they cannot understand.\n",
    "\n",
    "We can now train the classifier with this training_set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train vs test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train the classifer and then test its classifying ability on a brand new dataset that it has never seen before. \n",
    "\n",
    "Generally, a 80/20 ratio is a fair split between training and testing set:\n",
    "- training dataset (80% of the data)\n",
    "- testing dataset (20% of the data)\n",
    "\n",
    "Sklearn provides a function called train_test_split to do this easily. Let's split our feature_set into train_data and test_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in train data: 159\n",
      "Number of tweets in test data: 40\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(feature_set, test_size=0.20, train_size=0.80)\n",
    "print('Number of tweets in train data: {}'.format(len(train_data)))\n",
    "print('Number of tweets in test data: {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different types of model to use for classifying text data. The most common one is called Naive Bayesion Classifier and that is the one we are going to use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "         contains(bitch) = True                1 : 0      =      7.2 : 1.0\n",
      "          contains(fuck) = True                1 : 0      =      6.7 : 1.0\n",
      "     contains(terrorist) = True                1 : 0      =      4.7 : 1.0\n",
      "        contains(muslim) = True                1 : 0      =      4.1 : 1.0\n",
      "         contains(black) = True                1 : 0      =      3.4 : 1.0\n",
      "          contains(hate) = True                1 : 0      =      2.8 : 1.0\n",
      "           contains(get) = True                0 : 1      =      2.7 : 1.0\n",
      "        contains(immigr) = True                1 : 0      =      2.6 : 1.0\n",
      "          contains(scum) = True                1 : 0      =      2.2 : 1.0\n",
      "          contains(leav) = True                1 : 0      =      2.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayesian\n",
    "classifier1 = nltk.NaiveBayesClassifier.train(train_data)\n",
    "# SHOW FEATURES\n",
    "classifier1.show_most_informative_features(10)\n",
    "\n",
    "# Save the model into a pickle file\n",
    "f = open('classifier.pickle', 'wb')\n",
    "pickle.dump(classifier1, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! The model has been trained on the train_data.\n",
    "\n",
    "We can see which features the model considers important to decide between hateful speech and non-hateful speech.\n",
    "\n",
    "- Column 3 shows the ratio of occurence of each informative feature in both categories (hate vs nonhate).\n",
    "- Column 2 shows the direction of the ratio (which label occurs more frequently). Hate is 1, non-hate is 0. The label on the left is the label most associated with the corresponding feature.\n",
    "\n",
    "For example, tweets containing the word 'immigrants' are 2.6 times more likely to be hateful than not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the accuracy of our model on the test_data that we set aside earlier. These are tweets that the model has never seen before. We'll ask the model to classify them and see how its outcome compares with the true label of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy =  nltk.classify.util.accuracy(classifier1, test_data)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Use the classifier to identify hateful speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try our classifier on a new tweet new tweet of our choice. First we need to preprocess the tweet (clean, tokenize, stem and remove stopwords). Then we need to extract its features to look like the right input for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTweet = 'Hello world!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed tweet: ['hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Prepare the tweet\n",
    "def preprocessTweet(tweet):\n",
    "    \n",
    "    # clean the tweet\n",
    "    tweet = cleanTweet(testTweet)\n",
    "    \n",
    "    # tokenize the cleaned tweet\n",
    "    tokenised_tweet = nltk.word_tokenize(tweet)\n",
    "    \n",
    "    # remove stop words\n",
    "    tokenised_tweet_stpwd = [item for item in tokenised_tweet if item not in stopWords]\n",
    "    \n",
    "    # stem\n",
    "    pre_processed_tweet = [ps.stem(word) for word in tokenised_tweet_stpwd]\n",
    "    \n",
    "    print('Preprocessed tweet: {}'.format(pre_processed_tweet))\n",
    "    \n",
    "    return pre_processed_tweet\n",
    "\n",
    "preprocessed_tweet = preprocessTweet(testTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contains(fuck)': False, 'contains(you)': False, 'contains(off)': False, 'contains(go)': False, 'contains(get)': False, \"contains(('fuck', 'off'))\": False, 'contains(hate)': False, 'contains(paki)': False, 'contains(bastard)': False, 'contains(bitch)': False, 'contains(immigr)': False, 'contains(nigger)': False, 'contains(countri)': False, 'contains(polish)': False, 'contains(back)': False, 'contains(cunt)': False, 'contains(out)': False, 'contains(home)': False, 'contains(like)': False, 'contains(terrorist)': False, 'contains(muslim)': False, 'contains(faggot)': False, \"contains(('off', 'you'))\": False, 'contains(them)': False, 'contains(time)': False, 'contains(leav)': False, 'contains(right)': False, 'contains(black)': False, \"contains(('fuck', 'off', 'you'))\": False, 'contains(mani)': False, 'contains(peopl)': False, 'contains(work)': False, 'contains(make)': False, 'contains(let)': False, 'contains(good)': False, 'contains(love)': False, 'contains(daddi)': False, \"contains(('fuck', 'paki'))\": False, 'contains(come)': False, 'contains(job)': False, 'contains(even)': False, 'contains(brexit)': False, 'contains(road)': False, 'contains(shut)': False, 'contains(scum)': False, 'contains(want)': False, 'contains(today)': False, \"contains(('you', 'fuck'))\": False, 'contains(one)': False, 'contains(tell)': False}\n"
     ]
    }
   ],
   "source": [
    "# extract features\n",
    "tweet_feature_set = extract_features(preprocessed_tweet) \n",
    "print(tweet_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not hateful\n"
     ]
    }
   ],
   "source": [
    "# Classify\n",
    "verdict = classifier1.classify(tweet_feature_set)\n",
    "\n",
    "if verdict == 0:\n",
    "    print('Not hateful')\n",
    "else:\n",
    "    print('Hateful')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: try it!\n",
    "\n",
    "Make some of your own tweets and see whether or not they are hateful.\n",
    "\n",
    "Edit this code chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTweet = \"This is a test of hatefullness\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then run this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed tweet: ['test', 'hateful']\n",
      "Not hateful\n"
     ]
    }
   ],
   "source": [
    "preprocessed_tweet = preprocessTweet(testTweet)\n",
    "tweet_feature_set = extract_features(preprocessed_tweet) \n",
    "verdict = classifier1.classify(tweet_feature_set)\n",
    "\n",
    "if verdict == 0:\n",
    "    print('Not hateful')\n",
    "else:\n",
    "    print('Hateful with verdict '+str(verdict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
