{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate speech classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier to recognise hate speech on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to train a 'classifier' (a supervised machine learning algorithm) to recognise hate speech in a tweet. This is a technique of **Natural Language Processing (NLP)** similar to Sentiment Analysis.\n",
    "\n",
    "This requires two phases:\n",
    "- 1. **Train the classifier** (show the algorithm examples of both hateful and non-hateful tweets)\n",
    "- 2. **Test the accuracy of the classifier**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import the libraries we are going to need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import operator \n",
    "import pickle\n",
    "\n",
    "import nltk # nltk 3.4\n",
    "print('The nltk version is {}.'.format(nltk.__version__))\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk import ngrams\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tell python where it can find the data files (they're in a `data` sub-directory of the current folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"C:/Users/mednche/Desktop/Hate-speech-twitter-NLP/\")\n",
    "os.chdir(os.getcwd()+\"/data/\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pre-labeled Twitter dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the dataset used to train and test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('TrainingTweets.csv', encoding='ISO-8859-1')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'data' a Pandas dataframe. Let's see what it looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last 5 rows of the dataframe\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can see that each tweet has already been manually labeled in the column 'class'.\n",
    "- 1 means hateful\n",
    "- 0 means non-hateful\n",
    "\n",
    "This label is essential for what we are going to do (supervised machine learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Ensure balance in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many hateful vs non-hateful tweets there are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_tweets = data[data[\"class\"] == 1]\n",
    "print(\"{} hateful tweets\".format(len(hate_tweets)))\n",
    "\n",
    "nonhate_tweets = data[data[\"class\"] == 0]\n",
    "print(\"{} non-hateful tweets\".format(len(nonhate_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonhate_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be twice as many non-hateful tweets. This might affect the way the algorithm learns. We need to make the dataset balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Select as many hateful as non-hateful tweets for an equal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = min(len(hate_tweets), len(nonhate_tweets))\n",
    "\n",
    "# shuffle the table of hateful and non hateful tweets\n",
    "hate_tweets = shuffle(hate_tweets)\n",
    "nonhate_tweets = shuffle(nonhate_tweets)\n",
    "\n",
    "data_balanced = hate_tweets[0:num].append(nonhate_tweets[0:num], ignore_index=True)\n",
    "\n",
    "print('Number of tweets in balanced dataset: {}'.format(len(data_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we'll perform some preprocessing on the column 'tweet'. It will do things like turn the text all into lowercase, get rid of urls, remove usernames, etc. Have a look at the comments in the code below to see precisely what it's doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(tweet):\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to ''\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    #Remove the RT before the @user \n",
    "    tweet = re.sub('rt','',tweet) \n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #Remove @username\n",
    "    tweet = re.sub('@[^\\s]+','',tweet) \n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Remove non ASCII characters (emojies)\n",
    "    tweet= re.sub(r'[^\\x00-\\x7F]+','', tweet)\n",
    "    #Remove punctuation \n",
    "    tweet = \"\".join(l for l in tweet if l not in string.punctuation)\n",
    "    #Trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    #Remove beginning and end space\n",
    "    tweet = tweet.strip()\n",
    "\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "#  apply cleaning function to each tweet of the pandas dataframe\n",
    "data_balanced['tweet'] = data_balanced['tweet'].apply(cleanTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete empty tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty tweets ('') by NA\n",
    "data_balanced['tweet'].replace('', np.nan, inplace=True)\n",
    "# Delete all NA rows\n",
    "data_balanced.dropna(subset=['tweet'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: How many empty tweets were removed in the process? (Hint: use the shape attribute of the pandas dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tokenise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, the text of each tweet is a string. We would like to separate each word in that string so the model can 'read' them separately. \n",
    "\n",
    "In NLP, this is called 'tokenising': each tweet (intially a string of text) is chopped into a list of tokens (i.e. a list of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokenised = data_balanced.copy()\n",
    "\n",
    "data_tokenised['tweet'] = data_tokenised['tweet'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokenised.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can see that there are many words in the tweets that don't bring any meaning such as 'it', 'i', 'of' 'to' etc. These are called stopwords and need to be removed so that the classifier can focus on words that matter when telling the difference between hate and non-hate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Import English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these generic English stopwords could actually be useful in our context of hatespeech. For example 'them', 'out', 'off' could all be part of sentences like 'f**k off'. We will take these out of the list of stopwords. Also, we'll add some words to the stopword list based on some common spelling errors we observed in the tweets ('youre', 'dont', 'us')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove these from stopwords\n",
    "item_to_delete = ['you', 'out', 'off', 'them', 'themselves', 'yourself', 'from', 'same']\n",
    "stopWords = [e for e in stops if e not in item_to_delete]\n",
    "\n",
    "# Add these to stopwords\n",
    "item_to_add = [\"youre\", \"r\", \"you're\", \"us\", \"doesnt\", \"im\", \"hes\", \"u\", \"ya\", \"ww\", \n",
    "               \"dont\", \"https\", \"aint\", \"theres\", \"shouldnt\", \"thats\", \"amp\", \"wudnt\", \n",
    "               \"gonna\", \"ur\", \"cant\"]\n",
    "for e in item_to_add:\n",
    "    stopWords.append(e)\n",
    "\n",
    "print(sorted(stopWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords from tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to remove these stopwords from the tweets of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the processed data so far\n",
    "data_tokenised_stpwd = data_tokenised.copy()\n",
    "\n",
    "# Apply function that removes stopwords.\n",
    "data_tokenised_stpwd['tweet'] = data_tokenised_stpwd['tweet'].apply(\n",
    "    lambda x: [item for item in x if item not in stopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokenised_stpwd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this list of tokens to the one we had prior to removing the stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, stemming is the process of turning words back into their stem, base or root form.\n",
    "\n",
    "Examples:\n",
    "- 'cats' --> 'cat'\n",
    "- 'fishing', 'fished' --> 'fish'\n",
    "\n",
    "This step is important so the classifier understands that the singular and the plural form of a noun carry a similar meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the processed data so far\n",
    "pre_processed_data = data_tokenised_stpwd.copy()\n",
    "\n",
    "ps = PorterStemmer() \n",
    "pre_processed_data['tweet'] = pre_processed_data['tweet'].apply(\n",
    "    lambda x: [ps.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_data['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the pre-processing of the 'tweet' column of the dataset. We now have tweets that have been cleaned, stemmed and tokenised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model to learn anything, we need to give it a set of criteria to use in deciding whether a tweet is hateful or not. This kind of criteria is known as **feature**. We can define one or more feature(s) to train our classifier.\n",
    "\n",
    "In Part 2., we'll see how to convert the words into features so that we can feed it to a classifier for training or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Prepare the data to train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What feature shall we give to the model?\n",
    "\n",
    "We could give it a list of key words but text cannot be used by machine learning models. They expect their input to be numeric. So we need to transform words into numeric features in a meaningful way. \n",
    "\n",
    "To do so, we are going to set a list of words/features (called vocabulary) and provide the classifier with boolean values indicating whether each feature of the vocabulary is present or not.\n",
    "\n",
    "It will look something like this:\n",
    "- 'bastard' : True (present)\n",
    "- 'road' : False (absent)\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by creating a vocabulary of features: a set of all words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [word for tweet in pre_processed_data['tweet'] for word in tweet]\n",
    "print('Vocabulary size: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vocabulary contains a list of all unique words in our pre-processed tweets. You'll notice some of the words don't look very english. It's because they are the stem of the initial word (recall the stemming process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of hate in the English language is more complex than just the presence of a word. Sometimes it's the combination of 2 or more words that becomes hateful. For example 'shut up', 'f**k off' or 'send them home'. \n",
    "\n",
    "In NLP, these combinations of 2 or more words are called ngrams:\n",
    "- bigram: ('back', 'off')\n",
    "- trigram: ('send', 'them', 'home')\n",
    "\n",
    "We need to add bigrams and trigrams to our vocabulary of features alongside single words/features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tweets):\n",
    "    all_words = []\n",
    "    for word_list in tweets:\n",
    "        # unigrams\n",
    "        all_words.extend(word_list)\n",
    "        \n",
    "        # bigrams\n",
    "        bigrams = list(ngrams(word_list, 2))\n",
    "        \n",
    "        #trigrams \n",
    "        trigrams = list(ngrams(word_list, 3))\n",
    "        \n",
    "        all_words.extend(bigrams)\n",
    "        all_words.extend(trigrams)\n",
    "    \n",
    "    return all_words\n",
    "\n",
    "vocab = get_vocabulary(pre_processed_data['tweet'])\n",
    "print('Vocabulary size: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: some of the tokens are duplicates. This is  because either (1) they are repeated within a tweet or (2) they are present in multiple tweets. \n",
    "\n",
    "Don't worry though, we'll get unique features out of it soon, before training the classifier! \n",
    "\n",
    "But first, let's look at how frequent each of the features of the vocabulary is in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that features are tokens (unigrams) or combination of tokens (ngrams).\n",
    "\n",
    "Let's have a look at how frequent each feature is in the dataset. (_If you can't see the graph, try running the code chunk again_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(vocab)\n",
    "fd.plot(20, cumulative=True)\n",
    "#fd.xlabel('Most common features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fd.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to select a sample of this vocabulary of features. We only want to keep the features that truely matter in identifying hate speech.\n",
    "\n",
    "This step is important to reduce the  running time of our model as well as improve its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping the 50 most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rare features are only present in one or two tweet. We know that these are not going to be very useful to teach the model to recognise hate speech.  \n",
    "\n",
    "Let's only keep the top 50 most frequent features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_features(wordlist, n):\n",
    "    fd = nltk.FreqDist(wordlist)\n",
    "    \n",
    "    word_features = sorted(fd.items(), key=operator.itemgetter(1), reverse=True)[0:n] \n",
    "    word_features = [i[0] for i in word_features ]\n",
    "    return word_features\n",
    "\n",
    "# Only keep the top 50 most frequent words\n",
    "chosen_features = get_word_features(vocab, 50)\n",
    "print('Number of chosen features: {}/{}'.format(len(chosen_features), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chosen_features[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input data for classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have chosen a sample of features that we think are important for the model to learn to identify hateful speech.\n",
    "\n",
    "However, at this stage the classfier won't be able to know which features are responsible for a tweet being labelled as 'hateful'. Is it because of the word 'road' or the word 'bastard' in that tweet?\n",
    "\n",
    "To be able to learn what counts as hateful and what doesn't, the classifier needs to know the 'hateful value' of each feature in the vocabulary.\n",
    "\n",
    "In short, we need to tell the model:\n",
    "- which features are typically present in hateful tweets and which are not,\n",
    "- which features are typically present in non-hateful tweets and which are not.\n",
    "\n",
    "This precious information is available in our dataset because it has been manually labelled. So far we have not used the 'class' column in our dataset. We are now going to make use of it!\n",
    "\n",
    "The idea is to tell the model: \n",
    "- for each hateful tweet: these are the features present, and the ones not present. \n",
    "- for each non-hateful tweet: these are the features present, and the ones not present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the features present in each tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    feature_set = {}\n",
    "    for feature in chosen_features:\n",
    "        feature_set['contains({})'.format(feature)] = (feature in document_words)\n",
    "    return feature_set\n",
    "\n",
    "tweets = [tuple(x) for x in pre_processed_data.values]\n",
    "\n",
    "feature_set = nltk.classify.apply_features(extract_features, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of tweets in training_set: {}'.format(len(feature_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets' look at the first tweet. Notice at the end, we see that this is a hateful tweet (label = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method is pretty simple. For each tweet, we are looping through our 50 chosen_features and setting a boolean to True if the tweet contains that feature, False otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is what the classifier needs as input. It is a series of 0s and 1s (numerical) as opposed to text data that they cannot understand.\n",
    "\n",
    "We can now train the classifier with this training_set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train vs test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train the classifer and then test its classifying ability on a brand new dataset that it has never seen before. \n",
    "\n",
    "Generally, a 80/20 ratio is a fair split between training and testing set:\n",
    "- training dataset (80% of the data)\n",
    "- testing dataset (20% of the data)\n",
    "\n",
    "Sklearn provides a function called train_test_split to do this easily. Let's split our feature_set into train_data and test_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(feature_set, test_size=0.20, train_size=0.80)\n",
    "print('Number of tweets in train data: {}'.format(len(train_data)))\n",
    "print('Number of tweets in test data: {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different types of model to use for classifying text data. The most common one is called Naive Bayesion Classifier and that is the one we are going to use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayesian\n",
    "classifier1 = nltk.NaiveBayesClassifier.train(train_data)\n",
    "# SHOW FEATURES\n",
    "classifier1.show_most_informative_features(10)\n",
    "\n",
    "# Save the model into a pickle file\n",
    "f = open('classifier.pickle', 'wb')\n",
    "pickle.dump(classifier1, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! The model has been trained on the train_data.\n",
    "\n",
    "We can see which features the model considers important to decide between hateful speech and non-hateful speech.\n",
    "\n",
    "- Column 3 shows the ratio of occurence of each informative feature in both categories (hate vs nonhate).\n",
    "- Column 2 shows the direction of the ratio (which label occurs more frequently). Hate is 1, non-hate is 0. The label on the left is the label most associated with the corresponding feature.\n",
    "\n",
    "For example, tweets containing the word 'immigrants' are <span style=\"color:red\">5.7 times</span> more likely to be hateful than not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the accuracy of our model on the test_data that we set aside earlier. These are tweets that the model has never seen before. We'll ask the model to classify them and see how its outcome compares with the true label of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy =  nltk.classify.util.accuracy(classifier1, test_data)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Use the classifier to identify hateful speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try our classifier on a new tweet of your choice. First we need to preprocess the tweet (clean, tokenize, stem and remove stopwords). Then we need to extract its features to look like the right input for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTweet = 'Hello world!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the tweet\n",
    "def preprocessTweet(tweet):\n",
    "    \n",
    "    # clean the tweet\n",
    "    tweet = cleanTweet(testTweet)\n",
    "    \n",
    "    # tokenize the cleaned tweet\n",
    "    tokenised_tweet = nltk.word_tokenize(tweet)\n",
    "    \n",
    "    # remove stop words\n",
    "    tokenised_tweet_stpwd = [item for item in tokenised_tweet if item not in stopWords]\n",
    "    \n",
    "    # stem\n",
    "    pre_processed_tweet = [ps.stem(word) for word in tokenised_tweet_stpwd]\n",
    "    \n",
    "    print('Preprocessed tweet: {}'.format(pre_processed_tweet))\n",
    "    \n",
    "    return pre_processed_tweet\n",
    "\n",
    "preprocessed_tweet = preprocessTweet(testTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "tweet_feature_set = extract_features(preprocessed_tweet) \n",
    "print(tweet_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify\n",
    "verdict = classifier1.classify(tweet_feature_set)\n",
    "\n",
    "if verdict == 0:\n",
    "    print('Not hateful')\n",
    "else:\n",
    "    print('Hateful')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: try it!\n",
    "\n",
    "Make some of your own tweets and see whether or not they are hateful.\n",
    "\n",
    "Edit this cod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTweet = \"This is a test of hatefullness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_tweet = preprocessTweet(testTweet)\n",
    "tweet_feature_set = extract_features(preprocessed_tweet) \n",
    "verdict = classifier1.classify(tweet_feature_set)\n",
    "\n",
    "if verdict == 0:\n",
    "    print('Not hateful')\n",
    "else:\n",
    "    print('Hateful with verdict '+str(verdict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
