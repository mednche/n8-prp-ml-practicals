{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing for Crime Analysis\n",
    "\n",
    "This workbook will take you through some common Natural Language Processing (NLP) tasks. Ultimately it will analyse some crime notes data in order to try to identify distinct categories of crime. I.e., from the crime notes can we distinguish different types of crime?\n",
    "\n",
    "**Required libraries**\n",
    "\n",
    "The code below requires the following third-party libraries:\n",
    "\n",
    " - pandas\n",
    " - nltk\n",
    " - gensim\n",
    " - matplotlib\n",
    "\n",
    "These can be installed using most python package managers, e.g.:\n",
    "\n",
    "`conda install pandas nltk gensim matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "The code in this document has been designed to read a comma-separated-values (csv) file that contains a column with some crime notes in it. There can be other columns in the data, and these will be ignored, but ee need to tell python what the name of that column is. (Leave the following line alone unless you are using your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crime_column = \"Crime Notes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to load the necessary python libraries. These are basically pieces of code that other people have written that do useful jobs (like reading files, analysing data, drawing graphs, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geonsm\\AppData\\Local\\conda\\conda\\envs\\PRP\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import nltk # for the natural language processing\n",
    "\n",
    "# Prepare a list of stopwords:\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\",\", \".\"]) # Also include comma and full stop\n",
    "stop_words = set(stop_words)\n",
    "\n",
    "stemmer = nltk.PorterStemmer() # A 'stemmer' for finding roots of words\n",
    "\n",
    "import string # Useful things to do with strings\n",
    "\n",
    "# For doing topic modelling:\n",
    "from gensim import corpora, models, similarities\n",
    "from itertools import chain\n",
    "\n",
    "# For making graphs:\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Reading the Data\n",
    "\n",
    "Now can read the data. the `pandas` library (which we have abreviated to `pd`) has some very usefunl functions for reading and writing data, including one called `read_csv`. We are going to store all of the data in a big 'DataFrame', called simply `df` (we could use any word to refer to the data, but 'df' is common, partly because it is quick to type!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/taxis-after_whitelisting.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python now has access to those data through the variable called `df`. Lets do a couple of things to check that everything is as expeced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns in the data are called: Index(['Unnamed: 0', 'Crime Category', 'Crime Notes'], dtype='object')\n",
      "There are 1059 rows\n",
      "Here is some more information about the data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1059 entries, 0 to 1058\n",
      "Data columns (total 3 columns):\n",
      "Unnamed: 0        1059 non-null int64\n",
      "Crime Category    1059 non-null object\n",
      "Crime Notes       1059 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 24.9+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"The columns in the data are called:\", df.columns)\n",
    "\n",
    "print(\"There are\", len(df), \"rows\")\n",
    "\n",
    "print(\"Here is some more information about the data:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Text\n",
    "\n",
    "Before actually doing any analysis of the text, it is first necessary to prepare it. The following will go through some of the typical procedures, but there are others that might be useful as well.\n",
    "\n",
    "To prepare the text, the following code defines a new _function_. This function will be given the notes of a crime event as input, and it will return a clean version as its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_crime_text(text):\n",
    "    \n",
    "    # Convert all of the text to lower case\n",
    "    text = text.lower()\n",
    "       \n",
    "    # Tokenize the text (split it into its constituent words)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove 'stop words' (words like 'and', 'but', etc)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 'Stem' the words\n",
    "    #tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    \n",
    "    # Finish by taking all the tokens,  putting them back together\n",
    "    # into a single long string of text, and returning this.\n",
    "    output = \" \".join(tokens)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `clean_crime_text` function is ready. Now we will create a new column in our DataFrame called `clean_crime` using the raw crime notes as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['clean_crime'] = df[crime_column].apply(clean_crime_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, sometimes it is useful to create a huge list that contains every word, regardless of which crime notes it is part of. Create that list now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36751 words in total\n"
     ]
    }
   ],
   "source": [
    "all_words = [] # a list that will store every word\n",
    "# Run through every row and add the individual words to the all_words list\n",
    "for row in df['clean_crime']:\n",
    "    all_words.extend(nltk.word_tokenize(row))\n",
    "\n",
    "# Convert this list into a format that the natural language processing\n",
    "# toolkit (NLTK) understands\n",
    "all_words = nltk.Text(all_words)\n",
    "\n",
    "print(\"Found {} words in total\".format(len(all_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Analysis\n",
    "\n",
    "Now the text is ready we can do some exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common words\n",
    "\n",
    "Do a frequency distribution showing the most commonly occuring words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('taxi', 2624),\n",
       " ('suspect', 1967),\n",
       " ('driver', 1679),\n",
       " ('victim', 1461),\n",
       " ('complainant', 874),\n",
       " ('fare', 816),\n",
       " ('vehicle', 540),\n",
       " ('suspects', 436),\n",
       " ('male', 378),\n",
       " ('stated', 351),\n",
       " ('phone', 350),\n",
       " ('get', 348),\n",
       " ('comp', 323),\n",
       " ('money', 322),\n",
       " ('pay', 314),\n",
       " ('police', 305),\n",
       " ('address', 299),\n",
       " ('female', 294),\n",
       " ('times', 293),\n",
       " ('causing', 283)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the frequencies of the words\n",
    "fd = nltk.FreqDist(all_words)\n",
    "\n",
    "# Show the most common twenty words\n",
    "fd.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity**: In the cell below, write some code that will show the **50** most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colocations\n",
    "\n",
    "Look for bigrams (pairs of words) that occur more frequently than expected. Find the 20 most common collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxi driver; times stated; city centre; driver picks; leeds city;\n",
      "mobile phone; without paying; home address; pay fare; good escape; get\n",
      "money; private hire; cash machine; argument ensues; causing damage;\n",
      "attempt pay; picks fare; calls police; time date; making attempt\n"
     ]
    }
   ],
   "source": [
    "all_words.collocations(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity**: In the cell below, write some code that will show the **50** most common collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concordance\n",
    "\n",
    "Concordance allows us to look at the words that surround a particular word (i.e. the context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 350 matches:\n",
      " makes pay taxi driver returned taxi phone calls called address taxi picked sus\n",
      "ey taxi driver made bank person left phone taxi later contacted taxi company sa\n",
      " company said taxi driver taxi found phone victim gets taxi leeds date stated n\n",
      "ds date stated next realises 's left phone taxi company report 4 later call mal\n",
      " report 4 later call male stating 's phone taxi driver wants get give money get\n",
      "taxi driver wants get give money get phone back victim refuses calls police sus\n",
      "taxi driver works amber cars victims phone dropped pocket taxi victim taxi comp\n",
      "5 minutes found suspect made victims phone taxi driver picks fare male driven h\n",
      "houlder causing injury suspect taken phone payment taxi payment fare taxi drive\n",
      " taxi payment fare taxi driver given phone back driven phone caller taxi compan\n",
      " taxi driver given phone back driven phone caller taxi company complainant taxi\n",
      "ek side nose head neck victim leaves phone taxi taxi driver witnesses next pass\n",
      "driver witnesses next passenger take phone leaving taxi phone handed police vic\n",
      "xt passenger take phone leaving taxi phone handed police victim taxi driver unk\n",
      "iver victim passenger victim alights phone driver starts leave victim knocks wi\n",
      "ts leave victim knocks window shouts phone driver making victim contacts taxi r\n",
      "d get suspect ( taxi driver ) knocks phone hand fall floor smash taxi driver ge\n",
      "er victim home address victim leaves phone car whilst house money pay fare retu\n",
      "pect got enough money pay taxi hands phone unknown taxi driver collect paid cal\n",
      "nknown taxi driver collect paid call phone speak taxi driver unknown suspect ph\n",
      "ne speak taxi driver unknown suspect phone states phone suspect caller caller c\n",
      " driver unknown suspect phone states phone suspect caller caller contact taxi d\n",
      " caller contact taxi driver believed phone complainant taxi driver collected fa\n",
      "y taxi driver asks complainant gives phone security whilst gets complainant goe\n",
      "y taxi driver asks complainant gives phone security whilst gets complainant goe\n"
     ]
    }
   ],
   "source": [
    "# Look at the text that occurs around the word 'phone'\n",
    "all_words.concordance('phone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity**: In the cell below, write some code that will show the context of the word '**take**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common contexts\n",
    "\n",
    "Find the contexts where the specified words appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxi_calls taxi_handed taxi_leaving taxi_victim taxi_got taxi_suspect\n",
      "taxi_gone hands_driver taxi_found taxi_customer taxi_turned\n",
      "taxi_complainant taxi_victims\n"
     ]
    }
   ],
   "source": [
    "all_words.common_contexts(['phone', 'driver'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity**: In the cell below, write some code that will show the common contexts of two words that you're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by Crime Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequencies by Crime Type\n",
    "\n",
    "You might have noticed that the data contain the crime type as an additional column. Lets have a quick look to see whether the most common words are different for the different crime types.\n",
    "\n",
    "_Note that if you are using your own data then the following might not work (either because your crime type column is called something other than '`Crime Category`', which is easy to fix, or because you don't have the crime categories at all._\n",
    "\n",
    "The data contains the following unique categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Violent Crime', 'Criminal Damage', 'Theft From Motor Vehicle', 'Robbery', 'Fraud & Forgery', 'Other Theft'}\n"
     ]
    }
   ],
   "source": [
    "print(set(df['Crime Category']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if there is a difference between 'Criminal Damage' and 'Other Theft'.\n",
    "\n",
    "First create two lists of all the words associated with reports of 'Criminal Damage' and 'Other Theft' (as we did before for _all_ words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4250 words in Criminal Damage and 14164 with Other Theft\n"
     ]
    }
   ],
   "source": [
    "# Create two new bags of words, as we did before, but this time only including\n",
    "# the words associated with reports of 'Criminal Damage' and 'Other Theft'.\n",
    "\n",
    "crim_damage = []\n",
    "other_theft = []\n",
    "for (i, row) in df.iterrows():\n",
    "    if row['Crime Category'] == 'Criminal Damage':\n",
    "        crim_damage.extend(nltk.word_tokenize(row['clean_crime']))\n",
    "    elif row['Crime Category'] == 'Other Theft':\n",
    "        other_theft.extend(nltk.word_tokenize(row['clean_crime']))\n",
    "crim_damage = nltk.Text(crim_damage)\n",
    "other_theft = nltk.Text(other_theft)\n",
    "\n",
    "print(\"Found {} words in Criminal Damage and {} with Other Theft\".format(\\\n",
    "    len(crim_damage), len(other_theft)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see what the frequency distributions of the words associated with those crime categories look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common words for Criminal Damage are:\n",
      " [('taxi', 254), ('suspect', 244), ('driver', 180), ('victim', 124), ('vehicle', 108), ('complainant', 96), ('causing', 87), ('fare', 86), ('male', 55), ('suspects', 53)]\n",
      "\n",
      "The most common words for Other Theft are:\n",
      " [('taxi', 1277), ('driver', 745), ('suspect', 584), ('fare', 401), ('victim', 386), ('complainant', 336), ('phone', 280), ('pay', 214), ('money', 186), ('address', 161)]\n"
     ]
    }
   ],
   "source": [
    "print(\"The most common words for Criminal Damage are:\\n\", \\\n",
    "      nltk.FreqDist(crim_damage).most_common(10) )\n",
    "\n",
    "print(\"\\nThe most common words for Other Theft are:\\n\", \\\n",
    "      nltk.FreqDist(other_theft).most_common(10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: what do you notice about the differences in the words that commonly appear in 'Criminal Damage' and 'Other Theft' crimes? Are these differences as you would expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colocations by Crime Type\n",
    "\n",
    "Briefly repeat the colocations analysis to see if word combinations are different for the two crime types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colocations for Criminal Damage\n",
      "taxi driver; causing damage; times stated; city centre; wing mirror;\n",
      "leeds city; private hire; petrol station; argument ensues; suspect\n",
      "makes\n",
      "\n",
      "Colocations for Other Theft\n",
      "taxi driver; city centre; without paying; get money; driver picks;\n",
      "times stated; leeds city; mobile phone; pay fare; home address\n"
     ]
    }
   ],
   "source": [
    "print(\"Colocations for Criminal Damage\")\n",
    "crim_damage.collocations(10)\n",
    "\n",
    "print(\"\\nColocations for Other Theft\")\n",
    "other_theft.collocations(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: again, do you notice any differences in the words that commonly appear together for the different crime types? Are these differences as you would expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Could the analysis of collocations and/or word frequencies be useful for other investigatory work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering / Topic Modelling\n",
    "\n",
    "One of the most popular uses of natural language processing (and machine learing in general) is _classification_. Classification is a form of machine learning that involves grouping events that are similar.\n",
    "\n",
    "Here, we can construct a classification of the crime notes to look for those that contain similar combinations of words. As a proof of concept, we will then compare these classifications to their underlying crime type to see if there are any differences. I.e. is the algorithm able to distinguish, based purely on the crme notes, the different types of crime?\n",
    "\n",
    "For the model used below, we need to tell it how many clusters we want to create (see `NUM_TOPICS` in the code below). Use six in this case because there are six different crime categories. To do this rigorously, we would experiment with different numbers of clusters to try to find an optimal number. Also, it is worth nothing that, strictly speaking, the algorithm doesn't actually assign an entry to a topic. Instead, it returns a _distirbution_ of topics (i.e. the probability of the crime belonging to each topic). If the algorithm has worked well, then the probability of one topic will be particularly high. If the probabilities are all similar, then the algorithm is struggling to find any particularly strong features that it can use to assign the particular crime to a single topic.\n",
    "\n",
    "Note that we will use a method called 'Latedn Dirichlet Allocation' which actually does something called 'topic mod_delling'. But it's basically classification. Don't worry too much about the code below. It isn't doing anything particularly complicated, but looks like it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, we need a list of words. The all_words variable that we've been\n",
    "# using so far is actually an nltk.Text object, which we can't use.\n",
    "# Instead, create a new 'list of lists' that stores the notes for each \n",
    "# individual crime separately as inner lists.\n",
    "all_words_list = []\n",
    "# Run through every row and add the individual words to the all_words list\n",
    "for row in df['clean_crime']:\n",
    "    all_words_list.append(nltk.word_tokenize(row))\n",
    "\n",
    "# Create a dictionary. This assigns a number to every unique word.\n",
    "id2word = corpora.Dictionary(all_words_list)\n",
    "\n",
    "# Creates the Bag of Word corpus (convert each offence (document) to bag of words).\n",
    "mm = [id2word.doc2bow(text) for text in all_words_list]\n",
    "\n",
    "# Trains the LDA model.\n",
    "NUM_TOPICS = 6\n",
    "lda = models.ldamodel.LdaModel(corpus=mm, id2word=id2word, \\\n",
    "    num_topics=NUM_TOPICS,                              update_every=1, chunksize=10000, passes=1)\n",
    "\n",
    "# Assigns the topics to the documents in corpus\n",
    "lda_corpus = lda[mm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have created a model called `lda`. For information, you can use the `help` function to learn more about what it can do. (Although these documents are often difficult to understand!). If you're feeling brave, run the chunk below to see what help the authods of the `lda` model have provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LdaModel in module gensim.models.ldamodel object:\n",
      "\n",
      "class LdaModel(gensim.interfaces.TransformationABC, gensim.models.basemodel.BaseTopicModel)\n",
      " |  The constructor estimates Latent Dirichlet Allocation model parameters based\n",
      " |  on a training corpus:\n",
      " |  \n",
      " |  >>> lda = LdaModel(corpus, num_topics=10)\n",
      " |  \n",
      " |  You can then infer topic distributions on new, unseen documents, with\n",
      " |  \n",
      " |  >>> doc_lda = lda[doc_bow]\n",
      " |  \n",
      " |  The model can be updated (trained) with new documents via\n",
      " |  \n",
      " |  >>> lda.update(other_corpus)\n",
      " |  \n",
      " |  Model persistency is achieved through its `load`/`save` methods.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LdaModel\n",
      " |      gensim.interfaces.TransformationABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      gensim.models.basemodel.BaseTopicModel\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, bow, eps=None)\n",
      " |      Return topic distribution for the given document `bow`, as a list of\n",
      " |      (topic_id, topic_probability) 2-tuples.\n",
      " |      \n",
      " |      Ignore topics with very low probability (below `eps`).\n",
      " |  \n",
      " |  __init__(self, corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01, random_state=None, ns_conf={}, minimum_phi_value=0.01, per_word_topics=False)\n",
      " |      If given, start training from the iterable `corpus` straight away. If not given,\n",
      " |      the model is left untrained (presumably because you want to call `update()` manually).\n",
      " |      \n",
      " |      `num_topics` is the number of requested latent topics to be extracted from\n",
      " |      the training corpus.\n",
      " |      \n",
      " |      `id2word` is a mapping from word ids (integers) to words (strings). It is\n",
      " |      used to determine the vocabulary size, as well as for debugging and topic\n",
      " |      printing.\n",
      " |      \n",
      " |      `alpha` and `eta` are hyperparameters that affect sparsity of the document-topic\n",
      " |      (theta) and topic-word (lambda) distributions. Both default to a symmetric\n",
      " |      1.0/num_topics prior.\n",
      " |      \n",
      " |      `alpha` can be set to an explicit array = prior of your choice. It also\n",
      " |      support special values of 'asymmetric' and 'auto': the former uses a fixed\n",
      " |      normalized asymmetric 1.0/topicno prior, the latter learns an asymmetric\n",
      " |      prior directly from your data.\n",
      " |      \n",
      " |      `eta` can be a scalar for a symmetric prior over topic/word\n",
      " |      distributions, or a vector of shape num_words, which can be used to\n",
      " |      impose (user defined) asymmetric priors over the word distribution.\n",
      " |      It also supports the special value 'auto', which learns an asymmetric\n",
      " |      prior over words directly from your data. `eta` can also be a matrix\n",
      " |      of shape num_topics x num_words, which can be used to impose\n",
      " |      asymmetric priors over the word distribution on a per-topic basis\n",
      " |      (can not be learned from data).\n",
      " |      \n",
      " |      Turn on `distributed` to force distributed computing (see the `web tutorial <http://radimrehurek.com/gensim/distributed.html>`_\n",
      " |      on how to set up a cluster of machines for gensim).\n",
      " |      \n",
      " |      Calculate and log perplexity estimate from the latest mini-batch every\n",
      " |      `eval_every` model updates (setting this to 1 slows down training ~2x;\n",
      " |      default is 10 for better performance). Set to None to disable perplexity estimation.\n",
      " |      \n",
      " |      `decay` and `offset` parameters are the same as Kappa and Tau_0 in\n",
      " |      Hoffman et al, respectively.\n",
      " |      \n",
      " |      `minimum_probability` controls filtering the topics returned for a document (bow).\n",
      " |      \n",
      " |      `random_state` can be a np.random.RandomState object or the seed for one\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> lda = LdaModel(corpus, num_topics=100)  # train model\n",
      " |      >>> print(lda[doc_bow]) # get topic probability distribution for a document\n",
      " |      >>> lda.update(corpus2) # update the LDA model with additional documents\n",
      " |      >>> print(lda[doc_bow])\n",
      " |      \n",
      " |      >>> lda = LdaModel(corpus, num_topics=50, alpha='auto', eval_every=5)  # train asymmetric alpha from data\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  bound(self, corpus, gamma=None, subsample_ratio=1.0)\n",
      " |      Estimate the variational bound of documents from `corpus`:\n",
      " |      E_q[log p(corpus)] - E_q[log q(corpus)]\n",
      " |      \n",
      " |      `gamma` are the variational parameters on topic weights for each `corpus`\n",
      " |      document (=2d matrix=what comes out of `inference()`).\n",
      " |      If not supplied, will be inferred from the model.\n",
      " |  \n",
      " |  clear(self)\n",
      " |      Clear model state (free up some memory). Used in the distributed algo.\n",
      " |  \n",
      " |  diff(self, other, distance='kullback_leibler', num_words=100, n_ann_terms=10, normed=True)\n",
      " |      Calculate difference topic2topic between two Lda models\n",
      " |      `other` instances of `LdaMulticore` or `LdaModel`\n",
      " |      `distance` is function that will be applied to calculate difference between any topic pair.\n",
      " |      Available values: `kullback_leibler`, `hellinger` and `jaccard`\n",
      " |      `num_words` is quantity of most relevant words that used if distance == `jaccard` (also used for annotation)\n",
      " |      `n_ann_terms` is max quantity of words in intersection/symmetric difference between topics (used for annotation)\n",
      " |      Returns a matrix Z with shape (m1.num_topics, m2.num_topics), where Z[i][j] - difference between topic_i and topic_j\n",
      " |      and matrix annotation with shape (m1.num_topics, m2.num_topics, 2, None),\n",
      " |      where:\n",
      " |      \n",
      " |          annotation[i][j] = [[`int_1`, `int_2`, ...], [`diff_1`, `diff_2`, ...]] and\n",
      " |          `int_k` is word from intersection of `topic_i` and `topic_j` and\n",
      " |          `diff_l` is word from symmetric difference of `topic_i` and `topic_j`\n",
      " |          `normed` is a flag. If `true`, matrix Z will be normalized\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> m1, m2 = LdaMulticore.load(path_1), LdaMulticore.load(path_2)\n",
      " |      >>> mdiff, annotation = m1.diff(m2)\n",
      " |      >>> print(mdiff) # get matrix with difference for each topic pair from `m1` and `m2`\n",
      " |      >>> print(annotation) # get array with positive/negative words for each topic pair from `m1` and `m2`\n",
      " |  \n",
      " |  do_estep(self, chunk, state=None)\n",
      " |      Perform inference on a chunk of documents, and accumulate the collected\n",
      " |      sufficient statistics in `state` (or `self.state` if None).\n",
      " |  \n",
      " |  do_mstep(self, rho, other, extra_pass=False)\n",
      " |      M step: use linear interpolation between the existing topics and\n",
      " |      collected sufficient statistics in `other` to update the topics.\n",
      " |  \n",
      " |  get_document_topics(self, bow, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n",
      " |      Return topic distribution for the given document `bow`, as a list of\n",
      " |      (topic_id, topic_probability) 2-tuples.\n",
      " |      \n",
      " |      Ignore topics with very low probability (below `minimum_probability`).\n",
      " |      \n",
      " |      If per_word_topics is True, it also returns a list of topics, sorted in descending order of most likely topics for that word.\n",
      " |      It also returns a list of word_ids and each words corresponding topics' phi_values, multiplied by feature length (i.e, word count)\n",
      " |  \n",
      " |  get_term_topics(self, word_id, minimum_probability=None)\n",
      " |      Returns most likely topics for a particular word in vocab.\n",
      " |  \n",
      " |  get_topic_terms(self, topicid, topn=10)\n",
      " |      Return a list of `(word_id, probability)` 2-tuples for the most\n",
      " |      probable words in topic `topicid`.\n",
      " |      \n",
      " |      Only return 2-tuples for the topn most probable words (ignore the rest).\n",
      " |  \n",
      " |  inference(self, chunk, collect_sstats=False)\n",
      " |      Given a chunk of sparse document vectors, estimate gamma (parameters\n",
      " |      controlling the topic weights) for each document in the chunk.\n",
      " |      \n",
      " |      This function does not modify the model (=is read-only aka const). The\n",
      " |      whole input chunk of document is assumed to fit in RAM; chunking of a\n",
      " |      large corpus must be done earlier in the pipeline.\n",
      " |      \n",
      " |      If `collect_sstats` is True, also collect sufficient statistics needed\n",
      " |      to update the model's topic-word distributions, and return a 2-tuple\n",
      " |      `(gamma, sstats)`. Otherwise, return `(gamma, None)`. `gamma` is of shape\n",
      " |      `len(chunk) x self.num_topics`.\n",
      " |      \n",
      " |      Avoids computing the `phi` variational parameter directly using the\n",
      " |      optimization presented in **Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001**.\n",
      " |  \n",
      " |  init_dir_prior(self, prior, name)\n",
      " |  \n",
      " |  log_perplexity(self, chunk, total_docs=None)\n",
      " |      Calculate and return per-word likelihood bound, using the `chunk` of\n",
      " |      documents as evaluation corpus. Also output the calculated statistics. incl.\n",
      " |      perplexity=2^(-bound), to log at INFO level.\n",
      " |  \n",
      " |  save(self, fname, ignore=['state', 'dispatcher'], separately=None, *args, **kwargs)\n",
      " |      Save the model to file.\n",
      " |      \n",
      " |      Large internal arrays may be stored into separate files, with `fname` as prefix.\n",
      " |      \n",
      " |      `separately` can be used to define which arrays should be stored in separate files.\n",
      " |      \n",
      " |      `ignore` parameter can be used to define which variables should be ignored, i.e. left\n",
      " |      out from the pickled lda model. By default the internal `state` is ignored as it uses\n",
      " |      its own serialisation not the one provided by `LdaModel`. The `state` and `dispatcher`\n",
      " |      will be added to any ignore parameter defined.\n",
      " |      \n",
      " |      \n",
      " |      Note: do not save as a compressed file if you intend to load the file back with `mmap`.\n",
      " |      \n",
      " |      Note: If you intend to use models across Python 2/3 versions there are a few things to\n",
      " |      keep in mind:\n",
      " |      \n",
      " |        1. The pickled Python dictionaries will not work across Python versions\n",
      " |        2. The `save` method does not automatically save all np arrays using np, only\n",
      " |           those ones that exceed `sep_limit` set in `gensim.utils.SaveLoad.save`. The main\n",
      " |           concern here is the `alpha` array if for instance using `alpha='auto'`.\n",
      " |      \n",
      " |      Please refer to the wiki recipes section (https://github.com/piskvorky/gensim/wiki/Recipes-&-FAQ#q9-how-do-i-load-a-model-in-python-3-that-was-trained-and-saved-using-python-2)\n",
      " |      for an example on how to work around these issues.\n",
      " |  \n",
      " |  show_topic(self, topicid, topn=10)\n",
      " |      Return a list of `(word, probability)` 2-tuples for the most probable\n",
      " |      words in topic `topicid`.\n",
      " |      \n",
      " |      Only return 2-tuples for the topn most probable words (ignore the rest).\n",
      " |  \n",
      " |  show_topics(self, num_topics=10, num_words=10, log=False, formatted=True)\n",
      " |      For `num_topics` number of topics, return `num_words` most significant words\n",
      " |      (10 words per topic, by default).\n",
      " |      \n",
      " |      The topics are returned as a list -- a list of strings if `formatted` is\n",
      " |      True, or a list of `(word, probability)` 2-tuples if False.\n",
      " |      \n",
      " |      If `log` is True, also output this result to log.\n",
      " |      \n",
      " |      Unlike LSA, there is no natural ordering between the topics in LDA.\n",
      " |      The returned `num_topics <= self.num_topics` subset of all topics is therefore\n",
      " |      arbitrary and may change between two LDA training runs.\n",
      " |  \n",
      " |  sync_state(self)\n",
      " |  \n",
      " |  top_topics(self, corpus, num_words=20)\n",
      " |      Calculate the Umass topic coherence for each topic. Algorithm from\n",
      " |      **Mimno, Wallach, Talley, Leenders, McCallum: Optimizing Semantic Coherence in Topic Models, CEMNLP 2011.**\n",
      " |  \n",
      " |  update(self, corpus, chunksize=None, decay=None, offset=None, passes=None, update_every=None, eval_every=None, iterations=None, gamma_threshold=None, chunks_as_numpy=False)\n",
      " |      Train the model with new documents, by EM-iterating over `corpus` until\n",
      " |      the topics converge (or until the maximum number of allowed iterations\n",
      " |      is reached). `corpus` must be an iterable (repeatable stream of documents),\n",
      " |      \n",
      " |      In distributed mode, the E step is distributed over a cluster of machines.\n",
      " |      \n",
      " |      This update also supports updating an already trained model (`self`)\n",
      " |      with new documents from `corpus`; the two models are then merged in\n",
      " |      proportion to the number of old vs. new documents. This feature is still\n",
      " |      experimental for non-stationary input streams.\n",
      " |      \n",
      " |      For stationary input (no topic drift in new documents), on the other hand,\n",
      " |      this equals the online update of Hoffman et al. and is guaranteed to\n",
      " |      converge for any `decay` in (0.5, 1.0>. Additionally, for smaller\n",
      " |      `corpus` sizes, an increasing `offset` may be beneficial (see\n",
      " |      Table 1 in Hoffman et al.)\n",
      " |      \n",
      " |      Args:\n",
      " |          corpus (gensim corpus): The corpus with which the LDA model should be updated.\n",
      " |      \n",
      " |          chunks_as_numpy (bool): Whether each chunk passed to `.inference` should be a np\n",
      " |              array of not. np can in some settings turn the term IDs\n",
      " |              into floats, these will be converted back into integers in\n",
      " |              inference, which incurs a performance hit. For distributed\n",
      " |              computing it may be desirable to keep the chunks as np\n",
      " |              arrays.\n",
      " |      \n",
      " |      For other parameter settings, see :class:`LdaModel` constructor.\n",
      " |  \n",
      " |  update_alpha(self, gammat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-document\n",
      " |      topic weights `alpha` given the last `gammat`.\n",
      " |  \n",
      " |  update_eta(self, lambdat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-topic\n",
      " |      word weights `eta` given the last `lambdat`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(fname, *args, **kwargs) from builtins.type\n",
      " |      Load a previously saved object from file (also see `save`).\n",
      " |      \n",
      " |      Large arrays can be memmap'ed back as read-only (shared memory) by setting `mmap='r'`:\n",
      " |      \n",
      " |          >>> LdaModel.load(fname, mmap='r')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.basemodel.BaseTopicModel:\n",
      " |  \n",
      " |  print_topic(self, topicno, topn=10)\n",
      " |      Return a single topic as a formatted string. See `show_topic()` for parameters.\n",
      " |      \n",
      " |      >>> lsimodel.print_topic(10, topn=5)\n",
      " |      '-0.340 * \"category\" + 0.298 * \"$M$\" + 0.183 * \"algebra\" + -0.174 * \"functor\" + -0.168 * \"operator\"'\n",
      " |  \n",
      " |  print_topics(self, num_topics=20, num_words=10)\n",
      " |      Alias for `show_topics()` that prints the `num_words` most\n",
      " |      probable words for `topics` number of topics to log.\n",
      " |      Set `topics=-1` to print all topics.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a topic model that is able to take some crime notes and put them into one of six different clusters. Lets test it with an arbitrary crime. The model will show us how likely the crime is to fit into one of the six topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.084209171663971197), (1, 0.084272755222934953), (2, 0.08388900831525499), (3, 0.084071947391653024), (4, 0.084588201379439204), (5, 0.57896891602674672)]\n"
     ]
    }
   ],
   "source": [
    "new_entry = ['moving vehicle', 'car', 'over 18', 'taxi driver', 'employee working in vehicle', 'stranger']\n",
    "print(lda[id2word.doc2bow(new_entry)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: which topic is the new arbitrary crime most strongly associated with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see which words or phrases characterise each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.075*\"taxi\" + 0.050*\"suspect\" + 0.048*\"driver\" + 0.032*\"complainant\" + 0.027*\"victim\" + 0.019*\"fare\" + 0.016*\"vehicle\" + 0.014*\"stated\" + 0.011*\"suspects\" + 0.011*\"times\"'),\n",
       " (1,\n",
       "  '0.077*\"taxi\" + 0.068*\"victim\" + 0.043*\"driver\" + 0.038*\"suspect\" + 0.019*\"fare\" + 0.016*\"comp\" + 0.015*\"vehicle\" + 0.011*\"gets\" + 0.011*\"male\" + 0.010*\"times\"'),\n",
       " (2,\n",
       "  '0.046*\"taxi\" + 0.045*\"complainant\" + 0.038*\"driver\" + 0.030*\"suspect\" + 0.029*\"comp\" + 0.029*\"suspects\" + 0.024*\"fare\" + 0.019*\"male\" + 0.017*\"vehicle\" + 0.014*\"pay\"'),\n",
       " (3,\n",
       "  '0.090*\"taxi\" + 0.056*\"driver\" + 0.052*\"victim\" + 0.033*\"suspect\" + 0.024*\"fare\" + 0.019*\"phone\" + 0.018*\"complainant\" + 0.015*\"money\" + 0.012*\"male\" + 0.010*\"get\"'),\n",
       " (4,\n",
       "  '0.077*\"suspect\" + 0.063*\"taxi\" + 0.041*\"driver\" + 0.028*\"victim\" + 0.026*\"fare\" + 0.024*\"complainant\" + 0.018*\"vehicle\" + 0.014*\"suspects\" + 0.011*\"stated\" + 0.011*\"police\"'),\n",
       " (5,\n",
       "  '0.086*\"suspect\" + 0.068*\"victim\" + 0.051*\"taxi\" + 0.031*\"driver\" + 0.019*\"vehicle\" + 0.018*\"fare\" + 0.013*\"suspects\" + 0.010*\"car\" + 0.010*\"stated\" + 0.009*\"causing\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: do you notice any obvious differences in the six topics? Might these tell you something about the type of crime that the notes are describing?\n",
    "\n",
    "Optional: run the chunk below to see the topics more clearly (this makes quite a long list).\n",
    "\n",
    "It is also possible to visualise the topics, but we don't do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC 0\n",
      "\ttaxi : 0.0746817749598921\n",
      "\tsuspect : 0.04958591861435974\n",
      "\tdriver : 0.0480272933223971\n",
      "\tcomplainant : 0.03153131306498431\n",
      "\tvictim : 0.027148823883387758\n",
      "\tfare : 0.01893639108298777\n",
      "\tvehicle : 0.015719338367294527\n",
      "\tstated : 0.013685527354738544\n",
      "\tsuspects : 0.011075976292190447\n",
      "\ttimes : 0.010837603331839939\n",
      "\taddress : 0.01072175438312481\n",
      "\tcausing : 0.010241457941916311\n",
      "\tphone : 0.009952260815519375\n",
      "\tleeds : 0.009574274326914125\n",
      "\ttaken : 0.009481883511945954\n",
      "\tget : 0.008732344588713553\n",
      "\tmale : 0.00859156469437841\n",
      "\tpay : 0.008552096521288216\n",
      "\tpassenger : 0.008475084704211227\n",
      "\thome : 0.007941126189019435\n",
      "\n",
      "TOPIC 1\n",
      "\ttaxi : 0.0773437162593795\n",
      "\tvictim : 0.06758166840565706\n",
      "\tdriver : 0.04297416244960859\n",
      "\tsuspect : 0.037908463004449715\n",
      "\tfare : 0.019070670305904848\n",
      "\tcomp : 0.015900203642021753\n",
      "\tvehicle : 0.015013212776471765\n",
      "\tgets : 0.010694955633882555\n",
      "\tmale : 0.010646298270124607\n",
      "\ttimes : 0.009678928843963475\n",
      "\tdrives : 0.009435219701798721\n",
      "\tget : 0.009180282316383114\n",
      "\tstated : 0.008875096563432884\n",
      "\trear : 0.008844088371414828\n",
      "\tsuspects : 0.008644589221803467\n",
      "\tfemale : 0.008238481024090505\n",
      "\tcausing : 0.008046118024800894\n",
      "\tpay : 0.007929089702326593\n",
      "\tpolice : 0.007440741096780193\n",
      "\tmakes : 0.007383990070764382\n",
      "\n",
      "TOPIC 2\n",
      "\ttaxi : 0.04575478870277023\n",
      "\tcomplainant : 0.04546646921346666\n",
      "\tdriver : 0.03806043667574196\n",
      "\tsuspect : 0.030332935979155203\n",
      "\tcomp : 0.028926938462408097\n",
      "\tsuspects : 0.028502471591071308\n",
      "\tfare : 0.02403265678056006\n",
      "\tmale : 0.01936786531259579\n",
      "\tvehicle : 0.016523010830232176\n",
      "\tpay : 0.013737098843244784\n",
      "\tget : 0.011984901049779478\n",
      "\tvictim : 0.011970434987808743\n",
      "\tpicks : 0.01109102182758464\n",
      "\ttakes : 0.010353234343995453\n",
      "\tback : 0.010281428049130445\n",
      "\tfemale : 0.009611803728745743\n",
      "\tphone : 0.009205204771132347\n",
      "\tmales : 0.008945265208041352\n",
      "\tlocus : 0.00862370832729937\n",
      "\tone : 0.00841701560652816\n",
      "\n",
      "TOPIC 3\n",
      "\ttaxi : 0.09001453617952261\n",
      "\tdriver : 0.05637737753834727\n",
      "\tvictim : 0.052212754754231726\n",
      "\tsuspect : 0.03322438370496129\n",
      "\tfare : 0.02392765912358396\n",
      "\tphone : 0.018576335945019957\n",
      "\tcomplainant : 0.01765800138400883\n",
      "\tmoney : 0.015076814833090823\n",
      "\tmale : 0.012266782059691344\n",
      "\tget : 0.010283138068735351\n",
      "\tpolice : 0.010229015085319806\n",
      "\tpay : 0.010039386694271861\n",
      "\tleeds : 0.00944716308158712\n",
      "\tgets : 0.008862745349275709\n",
      "\tfemale : 0.008745767959352302\n",
      "\tasks : 0.008080894897208443\n",
      "\tgo : 0.007913878068921907\n",
      "\troad : 0.007587550835170554\n",
      "\tback : 0.007551685558376331\n",
      "\tlocus : 0.006979957818248653\n",
      "\n",
      "TOPIC 4\n",
      "\tsuspect : 0.07711611233216896\n",
      "\ttaxi : 0.06321836769809457\n",
      "\tdriver : 0.04111555222702729\n",
      "\tvictim : 0.0283925125141333\n",
      "\tfare : 0.02556981124991582\n",
      "\tcomplainant : 0.02378688959641799\n",
      "\tvehicle : 0.01802104468238614\n",
      "\tsuspects : 0.014138436119069186\n",
      "\tstated : 0.011217610835918473\n",
      "\tpolice : 0.011064965988262714\n",
      "\tlocus : 0.010628341665393577\n",
      "\tcausing : 0.01009057728383019\n",
      "\ttimes : 0.009624369255259238\n",
      "\taddress : 0.009558486422029855\n",
      "\tback : 0.00938011316839743\n",
      "\tfemale : 0.009058896680125939\n",
      "\tmale : 0.008975708131314089\n",
      "\tcar : 0.008939011343194482\n",
      "\tget : 0.008388717393873143\n",
      "\tmales : 0.006757657454577316\n",
      "\n",
      "TOPIC 5\n",
      "\tsuspect : 0.08570026664637558\n",
      "\tvictim : 0.06774695164499252\n",
      "\ttaxi : 0.05144925760519121\n",
      "\tdriver : 0.03110697259489973\n",
      "\tvehicle : 0.019191199098212458\n",
      "\tfare : 0.018396172137770898\n",
      "\tsuspects : 0.012695455184997987\n",
      "\tcar : 0.010319614106265439\n",
      "\tstated : 0.0096028032993343\n",
      "\tcausing : 0.009132523621783123\n",
      "\tget : 0.00887385813004323\n",
      "\tpolice : 0.008378500219796357\n",
      "\ttimes : 0.0080956225547407\n",
      "\tcomplainant : 0.007878530287622039\n",
      "\tdoor : 0.0077306111166596005\n",
      "\tmoney : 0.00736255620544032\n",
      "\taggrieved : 0.007296176893628709\n",
      "\tmaking : 0.006865147909249955\n",
      "\tfemale : 0.006786284522463453\n",
      "\taddress : 0.006696818524616413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topicid in range(NUM_TOPICS):\n",
    "    terms = lda.get_topic_terms(topicid, topn=20)\n",
    "    print(\"TOPIC {}\".format(topicid))\n",
    "    for word_id, prob in terms:\n",
    "        print(\"\\t{} : {}\".format(id2word[word_id],prob))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now link the results of the topic models back to the original data so that we can see which topic has been assigned to each individual crime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the same way that we created the 'clean' column of crime text,\n",
    "# we define a function and then apply it to the crime notes\n",
    "\n",
    "def find_topic(crime_note):\n",
    "    words = nltk.word_tokenize(crime_note)\n",
    "    # Find the distribution of topics over this crime note\n",
    "    topic_distribution = lda[id2word.doc2bow(words)]\n",
    "    # Get the probabilities of each topic into their own list\n",
    "    probs = [probability for topicid, probability in topic_distribution]\n",
    "    # Find the most likey topic\n",
    "    max_topic = probs.index(max(probs))\n",
    "    # That is the topic to return\n",
    "    return max_topic\n",
    "    ## Now add all of the topic probabilities\n",
    "    #for topicid, probability in topic_distribution:\n",
    "    #    s += (\",\"+str(probability))\n",
    "\n",
    "df['topic'] = df['clean_crime'].apply(find_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see which topics are the most common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>355431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0\n",
       "topic            \n",
       "0          355431\n",
       "1          144622\n",
       "2           24274\n",
       "3           27790\n",
       "4            6613\n",
       "5            1481"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a pivot table\n",
    "pivot1 = pd.pivot_table(df, index=['topic'], aggfunc='sum')\n",
    "\n",
    "pivot1\n",
    "# Draw a bar chart\n",
    "#plt.bar(x=range(NUM_TOPICS), height=pivot1.iloc[:,0].values.tolist(),\\\n",
    "#        width=1/NUM_TOPICS, color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finallly, lets see whether the topics correspond to the crime types by creating a table that shows how many different types of crime are associated with each topic.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"6\" halign=\"left\">Unnamed: 0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crime Category</th>\n",
       "      <th>Criminal Damage</th>\n",
       "      <th>Fraud &amp; Forgery</th>\n",
       "      <th>Other Theft</th>\n",
       "      <th>Robbery</th>\n",
       "      <th>Theft From Motor Vehicle</th>\n",
       "      <th>Violent Crime</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>29.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Unnamed: 0                                      \\\n",
       "Crime Category Criminal Damage Fraud & Forgery Other Theft Robbery   \n",
       "topic                                                                \n",
       "0                          7.3             2.1        29.4     1.8   \n",
       "1                          2.1             0.3        12.1     1.0   \n",
       "2                          0.4             0.0         1.5     0.4   \n",
       "3                          0.3             0.0         4.0     0.1   \n",
       "4                          0.0             0.0         0.9     0.0   \n",
       "5                          0.0             0.0         0.1     0.0   \n",
       "\n",
       "                                                       \n",
       "Crime Category Theft From Motor Vehicle Violent Crime  \n",
       "topic                                                  \n",
       "0                                   1.0          21.9  \n",
       "1                                   0.7           9.7  \n",
       "2                                   0.0           2.0  \n",
       "3                                   0.3           0.3  \n",
       "4                                   0.0           0.3  \n",
       "5                                   0.0           0.1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a pivot table that counts the number of crime categories per topic\n",
    "\n",
    "pivot2 = pd.pivot_table(df, index = ['topic'], columns = ['Crime Category'], aggfunc='sum')\n",
    "\n",
    "# Calculate proportions\n",
    "_sum = sum(pivot2.sum()) # This is the sum of all cells\n",
    "pivot2 = pivot2.applymap(lambda x: round((100*x)/_sum,1) if x >0  else 0)\n",
    "\n",
    "# Show the table\n",
    "pivot2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Has it 'worked'? Do the topics adequately distinguishable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Can you see any uses for clustering / topic modelling in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Another popular uses of natural language processing (and machine learing in general) is _classification_. Classification is a supervised form of machine learning. It reads data that have already been classified and tries to learn the patterns that lead to a particular classification. This is useful for classifying new data that we don't already have a classification for.\n",
    "\n",
    "Classification could be useful in the analysis of crime data by attempting to identify crimes, from their notes, that don't have their own classification already. E.g. trying to find crimes that are associated with a journey in a taxi. A human could begin by manually identifying and classifying a few hundred individual crimes, and the algorithm could then run through the rest of the data looking for crimes with similar characteristics.\n",
    "\n",
    "Another use, as discussed during the presentation, could be to classify text on social media by whether it is _hateful_ or not.\n",
    "\n",
    "We don't have the time to run through an example of classification as well, but there are plenty of examples online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: Whitelisting Crime Notes\n",
    "\n",
    "Without manually editting crime notes, it can be difficult to ensure that they are anonymous. It is easy to accidentally miss someone's name, an address, or some unique details. One way to reduce the risks of releasing identifying information is to **whitelist** the notes. In effect, this means looking at all of the unique words that appear in the data and removing all but the most common ones. As it happens, the few most common words often account for a very large portion of the total text, so removing the others shoudn't affect the natural language processing.\n",
    "\n",
    "The following code demonstrates how to do some simple whitelisting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the column with crime notes in\n",
    "crime_col = 'Crime Notes'\n",
    "\n",
    "# Read the data:\n",
    "raw_data = pd.read_csv('data/taxis-before_whitelisting.csv')\n",
    "\n",
    "# Tokenize and add to a big bag of all words\n",
    "all_words = []\n",
    "for index, row in raw_data.iterrows():\n",
    "    text = row[crime_col].lower() # Get the crime notes for this row and make lower case\n",
    "    tokens = nltk.word_tokenize(text) # Tokenize the crime notes\n",
    "    all_words.extend(tokens) # Add them to the big list of words\n",
    "\n",
    "# Create a big bag of words\n",
    "text = nltk.Text(all_words)\n",
    "\n",
    "# Count the frequencies of the words\n",
    "fd = nltk.FreqDist(text)\n",
    "\n",
    "# Display the most common words, their count, and their proportion,\n",
    "# stopping when the list of words accounts for 90% of all words\n",
    "# Also store these words in a 'whitelist'\n",
    "whitelist = set()\n",
    "cumulative = 0.0 # keep track of the cumulative percentage\n",
    "for i, (word, count) in enumerate(fd.most_common(1000)):\n",
    "    whitelist.add(word)\n",
    "    prop = count/len(all_words)*100\n",
    "    cumulative += prop\n",
    "    print(\"{i} {word} -> {count}, {proportion}, {cumulative}\".format(\\\n",
    "      i=i, word=word, count=count, proportion=prop, cumulative=cumulative))\n",
    "    if cumulative > 90:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a potential whitelist of words. Go through and make sure that they're OK, removing any that are sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_to_remove = [] # Add any extra words here\n",
    "whitelist = [word for word in whitelist if word not in words_to_remove]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finall go back to the original data and remove any words that are not in the whitelist. Then save the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame which will have only the whitelist words\n",
    "white_data = pd.DataFrame().reindex_like(raw_data)\n",
    "\n",
    "# Go through each row of the original data, clean the crime notes colum,\n",
    "# and then add the new row to the white_data\n",
    "for index, row in raw_data.iterrows():\n",
    "    text = row[crime_col].lower() # Get the crime notes for this row\n",
    "    tokens = nltk.word_tokenize(text) # Tokenize the crime notes\n",
    "    tokens = [t for t in tokens if t in whitelist]\n",
    "    white_text = \" \".join(tokens)\n",
    "    white_data.loc[index] = row.values.tolist()\n",
    "    white_data.loc[index,crime_col] = white_text\n",
    "    \n",
    "white_data.to_csv('data/taxis-after_whitelisting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
