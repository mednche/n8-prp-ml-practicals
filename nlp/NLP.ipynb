{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing for Crime Analysis\n",
    "\n",
    "This workbook will take you through some common Natural Language Processing (NLP) tasks. Ultimately it will analyse some crime notes data in order to try to identify distinct categories of crime. I.e., from the crime notes can we distinguish different types of crime?\n",
    "\n",
    "**Required libraries**\n",
    "\n",
    "The code below requires the following third-party libraries:\n",
    "\n",
    " - pandas\n",
    " - nltk\n",
    " - gensim\n",
    " - matplotlib\n",
    "\n",
    "These can be installed using most python package managers, e.g.:\n",
    "\n",
    "`conda install pandas nltk gensim matplotlib`\n",
    "\n",
    "The easy way to install all of the required packages is to import the [n8prp-environment.yml](../n8prp-environment.yml) file into Anaconda as per the instructions outlined in the [slides](../machine-learning-slides.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "The code in this document has been designed to read a comma-separated-values (csv) file that contains a column with some crime notes in it. There can be other columns in the data, and these will be ignored, but we need to tell python what the name of that column is. \n",
    "\n",
    "**YOU CAN PROVIDE YOUR OWN DATA FOR THIS TASK.** If you want to do this, put the csv file in the `data` directory, and then change the two variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT THE NAME OF THE COLUMN WITH YOUR CRIME NOTES ON THE LINE BELOW:\n",
    "crime_column  = \"Crime Notes\"\n",
    "\n",
    "# INSERT THE NAME OF THE CSV FILE ON THE LINE BELOW. IT SHOULD BE IN THE 'data' DIRECTORY\n",
    "csv_file_name = \"taxis-after_whitelisting.csv\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to load the necessary python libraries. These are basically pieces of code that other people have written that do useful jobs (like reading files, analysing data, drawing graphs, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import nltk # for the natural language processing\n",
    "\n",
    "# Prepare a list of stopwords (may need to download these from the Internet)\n",
    "from nltk.corpus import stopwords\n",
    "try:\n",
    "    stop_words = stopwords.words('english')\n",
    "except LookupError:\n",
    "    print(\"First time running, will download the NLTK stopwords\")\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "stop_words.extend([\",\", \".\"]) # Also include comma and full stop\n",
    "stop_words = set(stop_words)\n",
    "\n",
    "stemmer = nltk.PorterStemmer() # A 'stemmer' for finding roots of words\n",
    "\n",
    "import string # Useful things to do with strings\n",
    "\n",
    "# For doing topic modelling:\n",
    "from gensim import corpora, models, similarities\n",
    "from itertools import chain\n",
    "\n",
    "# For making graphs:\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Reading the Data\n",
    "\n",
    "Now can read the data. the `pandas` library (which we have abreviated to `pd`) has some very usefunl functions for reading and writing data, including one called `read_csv`. We are going to store all of the data in a big 'DataFrame', called simply `df` (we could use any word to refer to the data, but 'df' is common, partly because it is quick to type!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/\"+csv_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python now has access to those data through the variable called `df`. Lets do a couple of things to check that everything is as expeced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The columns in the data are called:\", df.columns)\n",
    "\n",
    "print(\"There are\", len(df), \"rows\")\n",
    "\n",
    "print(\"Here is some more information about the data:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Text\n",
    "\n",
    "Before actually doing any analysis of the text, it is first necessary to prepare it. The following cells will go through some of the typical procedures, but there are others that might be useful as well.\n",
    "\n",
    "To prepare the text, the following code defines a new _function_. This function will be given the notes of a crime event as input, and it will return a clean version as its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_crime_text(text):\n",
    "    \n",
    "    # Convert all of the text to lower case\n",
    "    text = text.lower()\n",
    "       \n",
    "    # Tokenize the text (split it into its constituent words)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove 'stop words' (words like 'and', 'but', etc)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 'Stem' the words\n",
    "    #tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    \n",
    "    # Finish by taking all the tokens,  putting them back together\n",
    "    # into a single long string of text, and returning this.\n",
    "    output = \" \".join(tokens)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also check that the 'punkt tokenizer' has been downloaded. Don't worry about this, it's needed to convert words into 'tokens'. If we don't install download it now we get a very clear error later telling us that we need to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also see if the 'punkt' tokenizer has been downloaded (this is required to turn the words into 'tokens')\n",
    "try: \n",
    "    nltk.word_tokenize(\"testing\")\n",
    "    print(\"Punkt tokenizer already downloaded\")\n",
    "except LookupError:\n",
    "    print(\"Downloading the punkt tokenizer\")\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `clean_crime_text` function is ready. Now we will create a new column in our DataFrame called `clean_crime` using the raw crime notes as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_crime'] = df[crime_column].apply(clean_crime_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a new column for the 'clean' crime notes. Lets look at a few rows to see what they're like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3): # Loop over rows 1 to 3\n",
    "    print(\"Row \"+str(i)+\":\")\n",
    "    print(\"\\t\"+str(df.loc[[i],['Crime Category']].values[0]))\n",
    "    print(\"\\t\"+str(df.loc[i:,['Crime Notes']].values[0]))\n",
    "    print(\"\\t\"+str(df.loc[i:,['clean_crime']].values[0]))\n",
    "\n",
    "# By the way, this is a slightly easier way to show a few rows from the top and bottom\n",
    "# for the three main columns:\n",
    "#df.loc[:,['Crime Category', 'Crime Notes', 'clean_crime']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, sometimes it is useful to create a huge list that contains every word, regardless of which crime notes it is part of. Create that list now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [] # a list that will store every word\n",
    "# Run through every row and add the individual words to the all_words list\n",
    "for row in df['clean_crime']:\n",
    "    all_words.extend(nltk.word_tokenize(row))\n",
    "\n",
    "# Convert this list into a format that the natural language processing\n",
    "# toolkit (NLTK) understands\n",
    "all_words = nltk.Text(all_words)\n",
    "\n",
    "print(\"Found {} words in total\".format(len(all_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Analysis\n",
    "\n",
    "Now the text is ready we can do some exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common words\n",
    "\n",
    "Do a frequency distribution showing the most commonly occuring words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequencies of the words\n",
    "fd = nltk.FreqDist(all_words)\n",
    "\n",
    "# Show the most common twenty words\n",
    "fd.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity**: In the cell below, write some code that will show the **50** most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colocations\n",
    "\n",
    "Look for bigrams (pairs of words) that occur more frequently than expected. Find the 20 most common collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words.collocations(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: the code above might not work. You might get an error about \"too many values to unpack\". This is a known bug with the current version of the NLTK library, as documented [here](https://github.com/nltk/nltk/issues/2299). It should be fixed in the next release_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity**: In the cell below, write some code that will show the **50** most common collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concordance\n",
    "\n",
    "Concordance allows us to look at the words that surround a particular word (i.e. the context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the text that occurs around the word 'phone'\n",
    "all_words.concordance('phone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity**: In the cell below, write some code that will show the context of the word '**take**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common contexts\n",
    "\n",
    "Find the contexts where the specified words appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words.common_contexts(['phone', 'driver'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity**: In the cell below, write some code that will show the common contexts of two words that you're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by Crime Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequencies by Crime Type\n",
    "\n",
    "You might have noticed that the data contain the crime type as an additional column. Lets have a quick look to see whether the most common words are different for the different crime types.\n",
    "\n",
    "_Note that if you are using your own data then the following might not work (either because your crime type column is called something other than '`Crime Category`', which is easy to fix by replacing '`Crime Category`' in the line below with whatever your column is called, or because you don't have the crime categories at all._\n",
    "\n",
    "The data contains the following unique categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(df['Crime Category']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if there is a difference between 'Criminal Damage' and 'Other Theft'.\n",
    "\n",
    "_If you are using your own data then you might want to replace these two types in the text below with two crime types that appear in your data_\n",
    "\n",
    "First create two lists of all the words associated with reports of 'Criminal Damage' and 'Other Theft' (as we did before for _all_ words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new bags of words, as we did before, but this time only including\n",
    "# the words associated with reports of 'Criminal Damage' and 'Other Theft'.\n",
    "\n",
    "crim_damage = []\n",
    "other_theft = []\n",
    "for (i, row) in df.iterrows():\n",
    "    if row['Crime Category'] == 'Criminal Damage':\n",
    "        crim_damage.extend(nltk.word_tokenize(row['clean_crime']))\n",
    "    elif row['Crime Category'] == 'Other Theft':\n",
    "        other_theft.extend(nltk.word_tokenize(row['clean_crime']))\n",
    "crim_damage = nltk.Text(crim_damage)\n",
    "other_theft = nltk.Text(other_theft)\n",
    "\n",
    "print(\"Found {} words in Criminal Damage and {} with Other Theft\".format(\\\n",
    "    len(crim_damage), len(other_theft)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see what the frequency distributions of the words associated with those crime categories look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most common words for Criminal Damage are:\\n\", \\\n",
    "      nltk.FreqDist(crim_damage).most_common(10) )\n",
    "\n",
    "print(\"\\nThe most common words for Other Theft are:\\n\", \\\n",
    "      nltk.FreqDist(other_theft).most_common(10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: what do you notice about the differences in the words that commonly appear in 'Criminal Damage' and 'Other Theft' crimes? Are these differences as you would expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colocations by Crime Type\n",
    "\n",
    "Briefly repeat the colocations analysis to see if word combinations are different for the two crime types.\n",
    "\n",
    "_As before the colocations might not work at the moment_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Colocations for Criminal Damage\")\n",
    "crim_damage.collocations(10)\n",
    "\n",
    "print(\"\\nColocations for Other Theft\")\n",
    "other_theft.collocations(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: again, do you notice any differences in the words that commonly appear together for the different crime types? Are these differences as you would expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Could the analysis of collocations and/or word frequencies be useful for other investigatory work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering / Topic Modelling\n",
    "\n",
    "One of the most popular uses of natural language processing (and machine learing in general) is _classification_. Classification is a form of machine learning that involves grouping events that are similar.\n",
    "\n",
    "Here, we can construct a classification of the crime notes to look for those that contain similar combinations of words. As a proof of concept, we will then compare these classifications to their underlying crime type to see if there are any differences. I.e. is the algorithm able to distinguish, based purely on the crme notes, the different types of crime?\n",
    "\n",
    "For the model used below, we need to tell it how many clusters we want to create (`NUM_TOPICS`). We use six in this case because there are six different crime categories. To do this rigorously, we would experiment with different numbers of clusters to try to find an optimal number. Also, it is worth nothing that, strictly speaking, the algorithm doesn't actually assign an entry to a topic. Instead, it returns a _distirbution_ of topics (i.e. the probability of the crime belonging to each topic). If the algorithm has worked well, then the probability of one topic will be particularly high. If the probabilities are all similar, then the algorithm is struggling to find any particularly strong features that it can use to assign the particular crime to a single topic.\n",
    "\n",
    "Note that we will use a method called 'Latedn Dirichlet Allocation' which actually does something called 'topic modelling'. But it's basically classification. Don't worry too much about the code below. It isn't doing anything particularly complicated, but looks like it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need a list of words. The all_words variable that we've been\n",
    "# using so far is actually an nltk.Text object, which we can't use.\n",
    "# Instead, create a new 'list of lists' that stores the notes for each \n",
    "# individual crime separately as inner lists.\n",
    "all_words_list = []\n",
    "# Run through every row and add the individual words to the all_words list\n",
    "for row in df['clean_crime']:\n",
    "    all_words_list.append(nltk.word_tokenize(row))\n",
    "\n",
    "# Create a dictionary. This assigns a number to every unique word.\n",
    "id2word = corpora.Dictionary(all_words_list)\n",
    "\n",
    "# Creates the Bag of Word corpus (convert each offence (document) to bag of words).\n",
    "mm = [id2word.doc2bow(text) for text in all_words_list]\n",
    "\n",
    "# Trains the LDA model.\n",
    "NUM_TOPICS = 6\n",
    "lda = models.ldamodel.LdaModel(corpus=mm, id2word=id2word, \\\n",
    "    num_topics=NUM_TOPICS,                              update_every=1, chunksize=10000, passes=1)\n",
    "\n",
    "# Assigns the topics to the documents in corpus\n",
    "lda_corpus = lda[mm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have created a model called `lda`. For information, you can use the `help` function to learn more about what it can do. (Although these documents are often difficult to understand!). If you're feeling brave, run the chunk below to see what help the authods of the `lda` model have provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a topic model that is able to take some crime notes and put them into one of six different clusters. Lets test it with an arbitrary crime. The model will show us how likely the crime is to fit into one of the six topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_entry = ['moving vehicle', 'car', 'over 18', 'taxi driver', 'employee working in vehicle', 'stranger']\n",
    "print(lda[id2word.doc2bow(new_entry)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: which topic is the new arbitrary crime most strongly associated with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see which words or phrases characterise each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: do you notice any obvious differences in the six topics? Might these tell you something about the type of crime that the notes are describing?\n",
    "\n",
    "Optional: run the chunk below to see the topics more clearly (this makes quite a long list).\n",
    "\n",
    "It is also possible to visualise the topics, but we don't do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topicid in range(NUM_TOPICS):\n",
    "    terms = lda.get_topic_terms(topicid, topn=20)\n",
    "    print(\"TOPIC {}\".format(topicid))\n",
    "    for word_id, prob in terms:\n",
    "        print(\"\\t{} : {}\".format(id2word[word_id],prob))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now link the results of the topic models back to the original data so that we can see which topic has been assigned to each individual crime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the same way that we created the 'clean' column of crime text,\n",
    "# we define a function and then apply it to the crime notes\n",
    "\n",
    "def find_topic(crime_note):\n",
    "    words = nltk.word_tokenize(crime_note)\n",
    "    # Find the distribution of topics over this crime note\n",
    "    topic_distribution = lda[id2word.doc2bow(words)]\n",
    "    # Get the probabilities of each topic into their own list\n",
    "    probs = [probability for topicid, probability in topic_distribution]\n",
    "    # Find the most likey topic\n",
    "    max_topic = probs.index(max(probs))\n",
    "    # That is the topic to return\n",
    "    return max_topic\n",
    "    ## Now add all of the topic probabilities\n",
    "    #for topicid, probability in topic_distribution:\n",
    "    #    s += (\",\"+str(probability))\n",
    "\n",
    "df['topic'] = df['clean_crime'].apply(find_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see which topics are the most common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a pivot table\n",
    "pivot1 = pd.pivot_table(df, index=['topic'], aggfunc='sum')\n",
    "pivot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a bar chart\n",
    "plt.bar(x=range(NUM_TOPICS), height=pivot1.iloc[:,0].values.tolist(), \\\n",
    "        width=1/NUM_TOPICS, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finallly, lets see whether the topics correspond to the crime types by creating a table that shows how many different types of crime (as a percentage) are associated with each topic.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a pivot table that counts the number of crime categories per topic\n",
    "\n",
    "pivot2 = pd.pivot_table(df, index = ['topic'], columns = ['Crime Category'], aggfunc='sum')\n",
    "\n",
    "# Calculate proportions\n",
    "_sum = sum(pivot2.sum()) # This is the sum of all cells\n",
    "pivot2 = pivot2.applymap(lambda x: round((100*x)/_sum,1) if x >0  else 0)\n",
    "\n",
    "# Show the table\n",
    "pivot2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Has it 'worked'? Do the topics adequately distinguishable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Can you see any uses for clustering / topic modelling in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Another popular uses of natural language processing (and machine learing in general) is _classification_. Classification is a supervised form of machine learning. It reads data that have already been classified and tries to learn the patterns that lead to a particular classification. This is useful for classifying new data that we don't already have a classification for.\n",
    "\n",
    "Classification could be useful in the analysis of crime data by attempting to identify crimes, from their notes, that don't have their own classification already. E.g. trying to find crimes that are associated with a journey in a taxi. A human could begin by manually identifying and classifying a few hundred individual crimes, and the algorithm could then run through the rest of the data looking for crimes with similar characteristics.\n",
    "\n",
    "Another use, as discussed during the presentation, could be to classify text on social media by whether it is _hateful_ or not.\n",
    "\n",
    "We don't have the time to run through an example of classification as well, but there are plenty of examples online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: Whitelisting Crime Notes\n",
    "\n",
    "Without manually editting crime notes, it can be difficult to ensure that they are anonymous. It is easy to accidentally miss someone's name, an address, or some unique details. One way to reduce the risks of releasing identifying information is to **whitelist** the notes. In effect, this means looking at all of the unique words that appear in the data and removing all but the most common ones. As it happens, the few most common words often account for a very large portion of the total text, so removing the others shoudn't affect the natural language processing.\n",
    "\n",
    "The following code demonstrates how to do some simple whitelisting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, you need to insert the name of your csv file and the column name below:\n",
    "crime_col = \"XXXX\"   \n",
    "crime_file = \"XXXX\"\n",
    "\n",
    "# Read the data:\n",
    "raw_data = pd.read_csv('data/'+crime_file)\n",
    "\n",
    "# Tokenize and add to a big bag of all words\n",
    "all_words = []\n",
    "for index, row in raw_data.iterrows():\n",
    "    text = row[crime_col].lower() # Get the crime notes for this row and make lower case\n",
    "    tokens = nltk.word_tokenize(text) # Tokenize the crime notes\n",
    "    all_words.extend(tokens) # Add them to the big list of words\n",
    "\n",
    "# Create a big bag of words\n",
    "text = nltk.Text(all_words)\n",
    "\n",
    "# Count the frequencies of the words\n",
    "fd = nltk.FreqDist(text)\n",
    "\n",
    "# Display the most common words, their count, and their proportion,\n",
    "# stopping when the list of words accounts for 90% of all words\n",
    "# Also store these words in a 'whitelist'\n",
    "whitelist = set()\n",
    "cumulative = 0.0 # keep track of the cumulative percentage\n",
    "for i, (word, count) in enumerate(fd.most_common(1000)):\n",
    "    whitelist.add(word)\n",
    "    prop = count/len(all_words)*100\n",
    "    cumulative += prop\n",
    "    print(\"{i} {word} -> {count}, {proportion}, {cumulative}\".format(\\\n",
    "      i=i, word=word, count=count, proportion=prop, cumulative=cumulative))\n",
    "    if cumulative > 90:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a potential whitelist of words. Go through and make sure that they're OK, removing any that are sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = [] # Add any extra words here\n",
    "whitelist = [word for word in whitelist if word not in words_to_remove]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finall go back to the original data and remove any words that are not in the whitelist. Then save the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame which will have only the whitelist words\n",
    "white_data = pd.DataFrame().reindex_like(raw_data)\n",
    "\n",
    "# Go through each row of the original data, clean the crime notes colum,\n",
    "# and then add the new row to the white_data\n",
    "for index, row in raw_data.iterrows():\n",
    "    text = row[crime_col].lower() # Get the crime notes for this row\n",
    "    tokens = nltk.word_tokenize(text) # Tokenize the crime notes\n",
    "    tokens = [t for t in tokens if t in whitelist]\n",
    "    white_text = \" \".join(tokens)\n",
    "    white_data.loc[index] = row.values.tolist()\n",
    "    white_data.loc[index,crime_col] = white_text\n",
    "    \n",
    "white_data.to_csv('data/taxis-after_whitelisting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
